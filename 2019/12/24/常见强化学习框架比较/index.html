<!DOCTYPE html>



  


<html class="theme-next mist use-motion" lang="zh-Hans">
<head><meta name="generator" content="Hexo 3.8.0">
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="UTF-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
<meta name="theme-color" content="#222">









<meta http-equiv="Cache-Control" content="no-transform">
<meta http-equiv="Cache-Control" content="no-siteapp">
















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css">




  
  
  
  

  
    
    
  

  
    
      
    

    
  

  
    
      
    

    
  

  
    
      
    

    
  

  
    
      
    

    
  

  
    
    
    <link href="//fonts.googleapis.com/css?family=" source han serif", dengxian:300,300italic,400,400italic,700,700italic|"source dengxian:300,300italic,400,400italic,700,700italic|monaco, consolas:300,300italic,400,400italic,700,700italic&subset="latin,latin-ext"" rel="stylesheet" type="text/css">
  






<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css">

<link href="/css/main.css?v=5.1.4" rel="stylesheet" type="text/css">


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=5.1.4">


  <link rel="mask-icon" href="/images/logo.svg?v=5.1.4" color="#222">





  <meta name="keywords" content="外文翻译,强化学习,">





  <link rel="alternate" href="/atom.xml" title="我的博客" type="application/atom+xml">






<meta name="description" content="A Comparison of Reinforcement Learning Frameworks: Dopamine, RLLib, Keras-RL, Coach, TRFL, Tensorforce, Coach and more （原文中的视频和谷歌趋势图可到原文中查看） 文中未涉及的框架：  Baselines rlpyt  PS 有一篇相关论文A Survey on Deep Rein">
<meta name="keywords" content="外文翻译,强化学习">
<meta property="og:type" content="article">
<meta property="og:title" content="常见强化学习框架比较">
<meta property="og:url" content="http://zanderchang.github.io/2019/12/24/常见强化学习框架比较/index.html">
<meta property="og:site_name" content="我的博客">
<meta property="og:description" content="A Comparison of Reinforcement Learning Frameworks: Dopamine, RLLib, Keras-RL, Coach, TRFL, Tensorforce, Coach and more （原文中的视频和谷歌趋势图可到原文中查看） 文中未涉及的框架：  Baselines rlpyt  PS 有一篇相关论文A Survey on Deep Rein">
<meta property="og:locale" content="zh-Hans">
<meta property="og:image" content="https://img.shields.io/github/stars/openai/gym.svg?style=flat-square">
<meta property="og:image" content="https://img.shields.io/github/issues-closed-raw/openai/gym.svg?style=flat-square">
<meta property="og:image" content="https://img.shields.io/github/contributors/openai/gym.svg?style=flat-square">
<meta property="og:image" content="https://img.shields.io/github/last-commit/openai/gym.svg?style=flat-square">
<meta property="og:image" content="https://img.shields.io/github/commit-activity/y/openai/gym.svg?style=flat-square">
<meta property="og:image" content="https://img.shields.io/github/stars/google/dopamine.svg?style=flat-square">
<meta property="og:image" content="https://img.shields.io/github/issues-closed-raw/google/dopamine.svg?style=flat-square">
<meta property="og:image" content="https://img.shields.io/github/contributors/google/dopamine.svg?style=flat-square">
<meta property="og:image" content="https://img.shields.io/github/last-commit/google/dopamine.svg?style=flat-square">
<meta property="og:image" content="https://img.shields.io/github/commit-activity/y/google/dopamine.svg?style=flat-square">
<meta property="og:image" content="https://img.shields.io/github/stars/ray-project/ray.svg?style=flat-square">
<meta property="og:image" content="https://img.shields.io/github/issues-closed-raw/ray-project/ray.svg?style=flat-square">
<meta property="og:image" content="https://img.shields.io/github/contributors/ray-project/ray.svg?style=flat-square">
<meta property="og:image" content="https://img.shields.io/github/last-commit/ray-project/ray.svg?style=flat-square">
<meta property="og:image" content="https://img.shields.io/github/commit-activity/y/ray-project/ray.svg?style=flat-square">
<meta property="og:image" content="https://img.shields.io/github/stars/keras-rl/keras-rl.svg?style=flat-square">
<meta property="og:image" content="https://img.shields.io/github/issues-closed-raw/keras-rl/keras-rl.svg?style=flat-square">
<meta property="og:image" content="https://img.shields.io/github/contributors/keras-rl/keras-rl.svg?style=flat-square">
<meta property="og:image" content="https://img.shields.io/github/last-commit/keras-rl/keras-rl.svg?style=flat-square">
<meta property="og:image" content="https://img.shields.io/github/commit-activity/y/keras-rl/keras-rl.svg?style=flat-square">
<meta property="og:image" content="https://img.shields.io/github/stars/deepmind/trfl.svg?style=flat-square">
<meta property="og:image" content="https://img.shields.io/github/issues-closed-raw/deepmind/trfl.svg?style=flat-square">
<meta property="og:image" content="https://img.shields.io/github/contributors/deepmind/trfl.svg?style=flat-square">
<meta property="og:image" content="https://img.shields.io/github/last-commit/deepmind/trfl.svg?style=flat-square">
<meta property="og:image" content="https://img.shields.io/github/commit-activity/y/deepmind/trfl.svg?style=flat-square">
<meta property="og:image" content="https://img.shields.io/github/stars/tensorforce/tensorforce.svg?style=flat-square">
<meta property="og:image" content="https://img.shields.io/github/issues-closed-raw/tensorforce/tensorforce.svg?style=flat-square">
<meta property="og:image" content="https://img.shields.io/github/contributors/tensorforce/tensorforce.svg?style=flat-square">
<meta property="og:image" content="https://img.shields.io/github/last-commit/tensorforce/tensorforce.svg?style=flat-square">
<meta property="og:image" content="https://img.shields.io/github/commit-activity/y/tensorforce/tensorforce.svg?style=flat-square">
<meta property="og:image" content="https://img.shields.io/github/stars/facebookresearch/Horizon.svg?style=flat-square">
<meta property="og:image" content="https://img.shields.io/github/issues-closed-raw/facebookresearch/Horizon.svg?style=flat-square">
<meta property="og:image" content="https://img.shields.io/github/contributors/facebookresearch/Horizon.svg?style=flat-square">
<meta property="og:image" content="https://img.shields.io/github/last-commit/facebookresearch/Horizon.svg?style=flat-square">
<meta property="og:image" content="https://img.shields.io/github/commit-activity/y/facebookresearch/Horizon.svg?style=flat-square">
<meta property="og:image" content="https://img.shields.io/github/stars/NervanaSystems/coach.svg?style=flat-square">
<meta property="og:image" content="https://img.shields.io/github/issues-closed-raw/NervanaSystems/coach.svg?style=flat-square">
<meta property="og:image" content="https://img.shields.io/github/contributors/NervanaSystems/coach.svg?style=flat-square">
<meta property="og:image" content="https://img.shields.io/github/last-commit/NervanaSystems/coach.svg?style=flat-square">
<meta property="og:image" content="https://img.shields.io/github/commit-activity/y/NervanaSystems/coach.svg?style=flat-square">
<meta property="og:image" content="https://img.shields.io/github/stars/geek-ai/MAgent.svg?style=flat-square">
<meta property="og:image" content="https://img.shields.io/github/issues-closed-raw/geek-ai/MAgent.svg?style=flat-square">
<meta property="og:image" content="https://img.shields.io/github/contributors/geek-ai/MAgent.svg?style=flat-square">
<meta property="og:image" content="https://img.shields.io/github/last-commit/geek-ai/MAgent.svg?style=flat-square">
<meta property="og:image" content="https://img.shields.io/github/commit-activity/y/geek-ai/MAgent.svg?style=flat-square">
<meta property="og:image" content="https://img.shields.io/github/stars/tensorflow/agents.svg?style=flat-square">
<meta property="og:image" content="https://img.shields.io/github/issues-closed-raw/tensorflow/agents.svg?style=flat-square">
<meta property="og:image" content="https://img.shields.io/github/contributors/tensorflow/agents.svg?style=flat-square">
<meta property="og:image" content="https://img.shields.io/github/last-commit/tensorflow/agents.svg?style=flat-square">
<meta property="og:image" content="https://img.shields.io/github/commit-activity/y/tensorflow/agents.svg?style=flat-square">
<meta property="og:image" content="https://img.shields.io/github/stars/kengz/SLM-Lab.svg?style=flat-square">
<meta property="og:image" content="https://img.shields.io/github/issues-closed-raw/kengz/SLM-Lab.svg?style=flat-square">
<meta property="og:image" content="https://img.shields.io/github/contributors/kengz/SLM-Lab.svg?style=flat-square">
<meta property="og:image" content="https://img.shields.io/github/last-commit/kengz/SLM-Lab.svg?style=flat-square">
<meta property="og:image" content="https://img.shields.io/github/commit-activity/y/kengz/SLM-Lab.svg?style=flat-square">
<meta property="og:image" content="https://user-images.githubusercontent.com/8209263/49688875-87e56e80-facd-11e8-90be-9d6be7bace03.gif">
<meta property="og:image" content="https://img.shields.io/github/stars/VinF/deer.svg?style=flat-square">
<meta property="og:image" content="https://img.shields.io/github/issues-closed-raw/VinF/deer.svg?style=flat-square">
<meta property="og:image" content="https://img.shields.io/github/contributors/VinF/deer.svg?style=flat-square">
<meta property="og:image" content="https://img.shields.io/github/last-commit/VinF/deer.svg?style=flat-square">
<meta property="og:image" content="https://img.shields.io/github/commit-activity/y/VinF/deer.svg?style=flat-square">
<meta property="og:image" content="https://WinderResearch.com/a-comparison-of-reinforcement-learning-frameworks-dopamine-rllib-keras-rl-coach-trfl-tensorforce-coach-and-more/./deer.png">
<meta property="og:image" content="https://img.shields.io/github/stars/rlworkgroup/garage.svg?style=flat-square">
<meta property="og:image" content="https://img.shields.io/github/issues-closed-raw/rlworkgroup/garage.svg?style=flat-square">
<meta property="og:image" content="https://img.shields.io/github/contributors/rlworkgroup/garage.svg?style=flat-square">
<meta property="og:image" content="https://img.shields.io/github/last-commit/rlworkgroup/garage.svg?style=flat-square">
<meta property="og:image" content="https://img.shields.io/github/commit-activity/y/rlworkgroup/garage.svg?style=flat-square">
<meta property="og:image" content="https://img.shields.io/github/stars/SurrealAI/surreal.svg?style=flat-square">
<meta property="og:image" content="https://img.shields.io/github/issues-closed-raw/SurrealAI/surreal.svg?style=flat-square">
<meta property="og:image" content="https://img.shields.io/github/contributors/SurrealAI/surreal.svg?style=flat-square">
<meta property="og:image" content="https://img.shields.io/github/last-commit/SurrealAI/surreal.svg?style=flat-square">
<meta property="og:image" content="https://img.shields.io/github/commit-activity/y/SurrealAI/surreal.svg?style=flat-square">
<meta property="og:image" content="https://img.shields.io/github/stars/rlgraph/rlgraph.svg?style=flat-square">
<meta property="og:image" content="https://img.shields.io/github/issues-closed-raw/rlgraph/rlgraph.svg?style=flat-square">
<meta property="og:image" content="https://img.shields.io/github/contributors/rlgraph/rlgraph.svg?style=flat-square">
<meta property="og:image" content="https://img.shields.io/github/last-commit/rlgraph/rlgraph.svg?style=flat-square">
<meta property="og:image" content="https://img.shields.io/github/commit-activity/y/rlgraph/rlgraph.svg?style=flat-square">
<meta property="og:image" content="https://img.shields.io/github/stars/david-abel/simple_rl.svg?style=flat-square">
<meta property="og:image" content="https://img.shields.io/github/issues-closed-raw/david-abel/simple_rl.svg?style=flat-square">
<meta property="og:image" content="https://img.shields.io/github/contributors/david-abel/simple_rl.svg?style=flat-square">
<meta property="og:image" content="https://img.shields.io/github/last-commit/david-abel/simple_rl.svg?style=flat-square">
<meta property="og:image" content="https://img.shields.io/github/commit-activity/y/david-abel/simple_rl.svg?style=flat-square">
<meta property="og:updated_time" content="2020-01-06T08:48:56.259Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="常见强化学习框架比较">
<meta name="twitter:description" content="A Comparison of Reinforcement Learning Frameworks: Dopamine, RLLib, Keras-RL, Coach, TRFL, Tensorforce, Coach and more （原文中的视频和谷歌趋势图可到原文中查看） 文中未涉及的框架：  Baselines rlpyt  PS 有一篇相关论文A Survey on Deep Rein">
<meta name="twitter:image" content="https://img.shields.io/github/stars/openai/gym.svg?style=flat-square">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Mist',
    version: '5.1.4',
    sidebar: {"position":"left","display":"post","offset":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    duoshuo: {
      userId: '0',
      author: '博主'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="http://zanderchang.github.io/2019/12/24/常见强化学习框架比较/">





  <title>常见强化学习框架比较 | 我的博客</title><!-- hexo-inject:begin --><!-- hexo-inject:end -->
  








</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-Hans">

  
  
    
  

  <!-- hexo-inject:begin --><!-- hexo-inject:end --><div class="container sidebar-position-left page-post-detail">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">我的博客</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle"></p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br>
            
            首页
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br>
            
            标签
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/categories/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-th"></i> <br>
            
            分类
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br>
            
            归档
          </a>
        </li>
      

      
        <li class="menu-item menu-item-search">
          
            <a href="javascript:;" class="popup-trigger">
          
            
              <i class="menu-item-icon fa fa-search fa-fw"></i> <br>
            
            搜索
          </a>
        </li>
      
    </ul>
  

  
    <div class="site-search">
      
  <div class="popup search-popup local-search-popup">
  <div class="local-search-header clearfix">
    <span class="search-icon">
      <i class="fa fa-search"></i>
    </span>
    <span class="popup-btn-close">
      <i class="fa fa-times-circle"></i>
    </span>
    <div class="local-search-input-wrapper">
      <input autocomplete="off" placeholder="搜索..." spellcheck="false" type="text" id="local-search-input">
    </div>
  </div>
  <div id="local-search-result"></div>
</div>



    </div>
  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://zanderchang.github.io/2019/12/24/常见强化学习框架比较/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Glory">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="我的博客">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">常见强化学习框架比较</h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2019-12-24T11:56:58+08:00">
                2019-12-24 11:56:58
              </time>
            

            
              <span class="post-meta-divider">|</span>
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-check-o"></i>
              </span>
              
                <span class="post-meta-item-text">更新于&#58;</span>
              
              <time title="更新于" itemprop="dateModified" datetime="2020-01-06T16:48:56+08:00">
                2020-01-06 16:48:56
              </time>
            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/强化学习/" itemprop="url" rel="index">
                    <span itemprop="name">强化学习</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        <p><em>A Comparison of Reinforcement Learning Frameworks: Dopamine, RLLib, Keras-RL, Coach, TRFL, Tensorforce, Coach and more</em></p>
<p>（原文中的视频和谷歌趋势图可到原文中查看）</p>
<p>文中未涉及的框架：</p>
<ul>
<li><a href="https://github.com/openai/baselines" target="_blank" rel="noopener">Baselines</a></li>
<li><a href="https://github.com/astooke/rlpyt" target="_blank" rel="noopener">rlpyt</a></li>
</ul>
<p>PS 有一篇相关论文<a href="http://www.koreascience.or.kr/article/JAKO201974757494930.page" target="_blank" rel="noopener">A Survey on Deep Reinforcement Learning Libraries</a>，韩文</p>
<a id="more"></a>
<p><em>Reinforcement Learning</em> (RL) frameworks help engineers by creating higher level abstractions of the core components of an RL algorithm. This makes code easier to develop, easier to read and improves efficiency.</p>
<p>But choosing a framework introduces some amount of lock in. An investment in learning and using a framework can make it hard to break away. This is just like when you decide which pub to visit. It’s very difficult not to buy a beer, no matter how bad the place is.</p>
<p>In this post I provide some notes about the most popular RL frameworks available. I also present some crude summary statistics from Github and Google (which you can’t trust) to attempt to quantify their popularity.</p>
<p>I believe posts like this are useful, but sometimes people are upset because I don’t like something about their framework. I do not want to upset anyone. If I do, I’m sorry. The work performed by the framework authors is nothing less than extraordinary. This is my meagre interpretation using the little time I have available. Please <a href="https://WinderResearch.com/contact/" target="_blank" rel="noopener">contact me</a> to make corrections.</p>
<h1 id="Original-Purpose-of-this-Work"><a href="#Original-Purpose-of-this-Work" class="headerlink" title="Original Purpose of this Work"></a>Original Purpose of this Work</h1><p>I am <a href="https://WinderResearch.com/announcement-new-reinforcement-learning-book-with-oreilly/" target="_blank" rel="noopener">writing a book on RL for O’Reilly</a>. As a part of that book, I want to demonstrate to my readers how to build and design various RL agents. I think the readers will benefit by using code from an already-established framework or library. And in any case, the people writing these frameworks would probably do a better job than I could anyway.</p>
<p>So the question was, “which framework?”. Which led me down this path. It started with a few frameworks, but then I found more. And more. And it turns out there are quite a lot of frameworks already available, which turned this into a 8000 word monster. Apologies in advance for the length. I don’t expect many people to read all of it!</p>
<p>Because of the length, this also took a while to write. This means that the reviews don’t have laser-focus. Sometimes I comment about one thing in one framework and not at all in another. Apologies for this; it was not intended to be exhaustive.</p>
<h1 id="Methodology"><a href="#Methodology" class="headerlink" title="Methodology"></a>Methodology</h1><p>Most of the evaluation is pure opinion. But there are a few quantitative metrics we can look at. Namely the statistics of the repository that are made available in Github. Starts roughly represent how well known each of the frameworks are, but they do not represent quality. More often than not the frameworks with the most stars have more marketing power.</p>
<p>After that, I was looking for a combination of modularity, ease of use, flexibility and maturity. Simplicity was also desired, but this is usually mutually exclusive with modularity and flexibility. The opinions presented below are based upon these ideals.</p>
<p>One reoccurring theme is the dominance of <em>Deep Learning</em> (DL) frameworks within the RL framework. Quite often the DL framework would breakout above the abstractions and the RL framework would just be an extension of the former. This means that if you are in a situation where you are already in bed with a particular DL solution, then you might as well stick with that.</p>
<p>But to me, that represents lock-in. My preference would always tend towards the frameworks that don’t mandate a specific DL implementation or don’t use DL at all (shock/horror!). The result is that all the Google frameworks tend towards Tensorflow, all the academic frameworks use PyTorch then there are a few brave souls dangling in-between with twice as much work as everyone else.</p>
<p>I also attempted to look at the Google rankings for each framework, but that <a href="##google-rankings">turned out to be unreliable</a>.</p>
<h1 id="Accompanying-Notebook"><a href="#Accompanying-Notebook" class="headerlink" title="Accompanying Notebook"></a>Accompanying Notebook</h1><p>Where I could, or where it made sense, I tried out a lot of these frameworks. Many of them didn’t work. And some of them had excellent Notebooks to begin with, so you can just check those out.</p>
<p>For the rest, I have <a href="https://colab.research.google.com/gist/philwinder/07cbe7b696745ac25c0f6a2aadbcd3c7/framework-research.ipynb" target="_blank" rel="noopener">published a gist that you can run on Google Colabratory</a>. This is presented in a very raw format. It is not meant to be comprehensive or explanatory. I simply wanted to double check that in the simplest of cases, it worked.</p>
<p>In each section I also present a “Getting Started” sub-heading that demonstrates the basic example from each framework. This code is from the Notebook.</p>
<h1 id="Reinforcement-Learning-Frameworks"><a href="#Reinforcement-Learning-Frameworks" class="headerlink" title="Reinforcement Learning Frameworks"></a>Reinforcement Learning Frameworks</h1><p>The following frameworks are listed in order of the number of stars in their Github repository as of June 2019. The actual number of stars and other metrics are presented as badges just below the title of each framework.</p>
<p>The following frameworks are compared:</p>
<ul>
<li><a href="##OpenAI\ Gym">OpenAI Gym</a></li>
<li><a href="##Google\ Dopamine">Google Dopamine</a></li>
<li><a href="##RLLib\ via\ ray-project">RLLib</a></li>
<li><a href="##Keras-RL">Keras-RL</a></li>
<li><a href="##TRFL">TRFL</a></li>
<li><a href="##Tensorforce">Tensorforce</a></li>
<li><a href="##Facebook\ Horizon">Facebook Horizon</a></li>
<li><a href="##Nervana\ Systems\ Coach">Nervana Systems Coach</a></li>
<li><a href="##MAgent">MAgent</a></li>
<li><a href="##SLM-Lab">SLM-Lab</a></li>
<li><a href="##DeeR">DeeR</a></li>
<li><a href="##Garage">Garage</a></li>
<li><a href="##Surreal">Surreal</a></li>
<li><a href="##RLgraph">RLgraph</a></li>
<li><a href="##Simple\ RL">Simple RL</a></li>
</ul>
<h2 id="OpenAI-Gym"><a href="#OpenAI-Gym" class="headerlink" title="OpenAI Gym"></a><a href="https://github.com/openai/gym" target="_blank" rel="noopener">OpenAI Gym</a></h2><p><img src="https://img.shields.io/github/stars/openai/gym.svg?style=flat-square" alt><br><img src="https://img.shields.io/github/issues-closed-raw/openai/gym.svg?style=flat-square" alt><br><img src="https://img.shields.io/github/contributors/openai/gym.svg?style=flat-square" alt><br><img src="https://img.shields.io/github/last-commit/openai/gym.svg?style=flat-square" alt><br><img src="https://img.shields.io/github/commit-activity/y/openai/gym.svg?style=flat-square" alt></p>
<p>OpenAI is a not-profit, pure research company. The provide a range of open-source Deep and Reinforcement Learning tools to improve repeatability, create benchmarks and improve upon the state of the art. I like to think of them as a bridge between academia and industry.</p>
<p>But I know what you’re thinking. “Phil, Gym is not a framework. It is an environment.”. I know, I know. It provides a range of toy environments, classic control, robotics, video games and board games to test your RL algorithm against.</p>
<p>But I have included it here because it is used so often as the basis for custom work. People use it like a framework. Think of it as an interface between an RL implementation and the environment. It is so prolific, many of the other frameworks listed below also interface with Gym. Furthermore it acts as a baseline as to compare everything against. Since this is one of the most popular repositories in RL.</p>
<h2 id="Getting-Started"><a href="#Getting-Started" class="headerlink" title="Getting Started"></a>Getting Started</h2><p>Gym is both cool and problematic because of it’s realistic 3D environments. If you want to visualise what is going on you need to be able to render these environments. It pretty much works on your laptop, but struggles when you try and run it in a Notebook because of limitations with the browser.</p>
<p>To work around this, you have to use a virtual display. Basically we have to mock out the video driver. This means most of the “getting started” code is video wrapping code.</p>
<p>If we ignore all the boring stuff, which you can find in the accompanying Notebook, the core gym code looks like:</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> gym</span><br><span class="line"><span class="keyword">from</span> gym.wrappers.monitoring.video_recorder <span class="keyword">import</span> VideoRecorder <span class="comment"># Because we want to record a video</span></span><br><span class="line"></span><br><span class="line">env = gym.make(<span class="string">"CartPole-v1"</span>) <span class="comment"># Create the cartpole environment</span></span><br><span class="line">rec = VideoRecorder(env)      <span class="comment"># Create the video recorder</span></span><br><span class="line">rec.capture_frame()           <span class="comment"># Capture the starting position</span></span><br><span class="line"><span class="keyword">while</span> <span class="literal">True</span>:</span><br><span class="line">    action = env.action_space.sample()                   <span class="comment"># Use a random action</span></span><br><span class="line">    observation, reward, done, info = env.step(action)   <span class="comment"># to take a single step in the environment</span></span><br><span class="line">    rec.capture_frame()                                  <span class="comment"># and record</span></span><br><span class="line">    <span class="keyword">if</span> done:</span><br><span class="line">           <span class="keyword">break</span>                                         <span class="comment"># If the pole has fallen, quit.</span></span><br><span class="line">rec.close()  <span class="comment"># Close the recording</span></span><br><span class="line">env.close()  <span class="comment"># Close the environment</span></span><br></pre></td></tr></table></figure>
<p>As you can see, it falls straight away because we’re just passing random actions at the moment. But still, there’s something hypnotic, something drum-and-bass about it.</p>
<p>But now let’s look at some agent framework only options.</p>
<h2 id="Google-Dopamine"><a href="#Google-Dopamine" class="headerlink" title="Google Dopamine"></a><a href="https://github.com/google/dopamine" target="_blank" rel="noopener">Google Dopamine</a></h2><p><img src="https://img.shields.io/github/stars/google/dopamine.svg?style=flat-square" alt><br><img src="https://img.shields.io/github/issues-closed-raw/google/dopamine.svg?style=flat-square" alt><br><img src="https://img.shields.io/github/contributors/google/dopamine.svg?style=flat-square" alt><br><img src="https://img.shields.io/github/last-commit/google/dopamine.svg?style=flat-square" alt><br><img src="https://img.shields.io/github/commit-activity/y/google/dopamine.svg?style=flat-square" alt></p>
<p>Google Dopamine: “Not an Official Google product” (NOGP - an acronym I’m going to coin now) but written by Google employees and hosted on Google github. So, Google Dopamine then. It is a relatively new entrant to the RL framework space that appears to have been a hit. It boasts a large number of Github stars and some amount of Google trend ranking. This is especially surprising because of the limited number of commits, committers and time since the project was launched. Clearly it helps you have Google’s branding and marketing department.</p>
<p>Anyway, the cool thing about this framework is that it emphasises configuration as code through it’s use of the <a href="https://github.com/google/gin-config" target="_blank" rel="noopener">Google gin-config</a> configuration framework. The idea is that you have lots of pluggable bits that you plumb together through a configuration file. The benefit is that this allows people to release a single configuration file that contains all of the parameters specific to that run. And gin-config makes things special because it allows you to plumb together objects; instances of classes and lambdas and things like that.</p>
<p>The downside is that you increase the complexity in the configuration file and it can end up like just another file full of code that people can’t understand because they are not used to it. Personally I would always stick to a “dumb” configuration file like Kubernetes Manifests or JSON (like many of the other frameworks), for example. The wiring should be done in the code.</p>
<p>The one major benefit is that it promotes plugability and reuse, which are key OOP and Functional concepts that are often ignored when developing <a href="https://WinderResearch.com/what-is-data-science/" target="_blank" rel="noopener">Data Science</a> products.</p>
<p>It is clear that it has gained a significant amount of traction in a very short time. And frankly, that worries me a bit. There are four contributors and only 100 commits. Of those four people, three are from the community (bug-fixes, etc.). This leaves one person. And this one person has committed, wait for it, <a href="https://github.com/google/dopamine/graphs/contributors" target="_blank" rel="noopener"><strong>over 1.3 million lines of code</strong></a>.</p>
<p>Clearly there’s something fishing going on here. From the commit history it looks like the code was <a href="https://github.com/google/dopamine/commit/420b147474d455fe39d911432fe579e54db3a1e0" target="_blank" rel="noopener">transferred from another repo</a>. A 1.2 million line commit isn’t exactly best practice! :-) It’s Apache licensed, so there’s nothing too strange going on but the copyright has been assigned to <a href="https://github.com/google/dopamine" target="_blank" rel="noopener">Google Inc.</a>. But I’m reassured by the <a href="https://github.com/google/dopamine/blob/master/CONTRIBUTING.md#contributor-license-agreement" target="_blank" rel="noopener">contributor agreement</a>.</p>
<p>In terms of modularity, there isn’t much. There isn’t any abstraction for the <a href="https://github.com/google/dopamine/tree/master/dopamine/agents" target="_blank" rel="noopener">Agents</a>; they are implemented directly and configured from the gin config. There’s not many implemented either. There isn’t any official abstraction of an environment either. In fact, it looks like they are just passing back core Tensorflow objects everywhere and assuming using the Tensorflow interface. In short, very little official OOP-style abstraction, which is different to most of the other frameworks.</p>
<p>In short, little modularity, reuse is clunky (IMO) and although it appears to be popular, it isn’t very mature and doesn’t have community support.</p>
<h3 id="Getting-Started-1"><a href="#Getting-Started-1" class="headerlink" title="Getting Started"></a>Getting Started</h3><p>Again you can find the example in the accompanying Notebook, but the premise is to build your RL algorithm via a configuration file. This is what it looks like:</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line">DQN_PATH = os.path.join(BASE_PATH, <span class="string">'dqn'</span>)</span><br><span class="line"><span class="comment"># Modified from dopamine/agents/dqn/config/dqn_cartpole.gin</span></span><br><span class="line">dqn_config = <span class="string">"""</span></span><br><span class="line"><span class="string"># Hyperparameters for a simple DQN-style Cartpole agent. The hyperparameters</span></span><br><span class="line"><span class="string"># chosen achieve reasonable performance.</span></span><br><span class="line"><span class="string">import dopamine.discrete_domains.gym_lib</span></span><br><span class="line"><span class="string">import dopamine.discrete_domains.run_experiment</span></span><br><span class="line"><span class="string">import dopamine.agents.dqn.dqn_agent</span></span><br><span class="line"><span class="string">import dopamine.replay_memory.circular_replay_buffer</span></span><br><span class="line"><span class="string">import gin.tf.external_configurables</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">DQNAgent.observation_shape = %gym_lib.CARTPOLE_OBSERVATION_SHAPE</span></span><br><span class="line"><span class="string">DQNAgent.observation_dtype = %gym_lib.CARTPOLE_OBSERVATION_DTYPE</span></span><br><span class="line"><span class="string">DQNAgent.stack_size = %gym_lib.CARTPOLE_STACK_SIZE</span></span><br><span class="line"><span class="string">DQNAgent.network = @gym_lib.cartpole_dqn_network</span></span><br><span class="line"><span class="string">DQNAgent.gamma = 0.99</span></span><br><span class="line"><span class="string">DQNAgent.update_horizon = 1</span></span><br><span class="line"><span class="string">DQNAgent.min_replay_history = 500</span></span><br><span class="line"><span class="string">DQNAgent.update_period = 4</span></span><br><span class="line"><span class="string">DQNAgent.target_update_period = 100</span></span><br><span class="line"><span class="string">DQNAgent.epsilon_fn = @dqn_agent.identity_epsilon</span></span><br><span class="line"><span class="string">DQNAgent.tf_device = '/gpu:0'  # use '/cpu:*' for non-GPU version</span></span><br><span class="line"><span class="string">DQNAgent.optimizer = @tf.train.AdamOptimizer()</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">tf.train.AdamOptimizer.learning_rate = 0.001</span></span><br><span class="line"><span class="string">tf.train.AdamOptimizer.epsilon = 0.0003125</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">create_gym_environment.environment_name = 'CartPole'</span></span><br><span class="line"><span class="string">create_gym_environment.version = 'v0'</span></span><br><span class="line"><span class="string">create_agent.agent_name = 'dqn'</span></span><br><span class="line"><span class="string">TrainRunner.create_environment_fn = @gym_lib.create_gym_environment</span></span><br><span class="line"><span class="string">Runner.num_iterations = 100</span></span><br><span class="line"><span class="string">Runner.training_steps = 100</span></span><br><span class="line"><span class="string">Runner.evaluation_steps = 100</span></span><br><span class="line"><span class="string">Runner.max_steps_per_episode = 200  # Default max episode length.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">WrappedReplayBuffer.replay_capacity = 50000</span></span><br><span class="line"><span class="string">WrappedReplayBuffer.batch_size = 128</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line">gin.parse_config(dqn_config, skip_unknown=<span class="literal">False</span>)</span><br></pre></td></tr></table></figure>
<p>That’s quite a lot. But it’s implementing a more complicated algorithm so we might expect that. I’m quite happy about the hyperparameters being in there, but I’m not sure that I am a fan of all the dynamic injection (the <code>@</code> denotes an instance of a class). Proponents would say that “wow, look, I can just swap out the optimiser just by changing this line”. But I’m of the opinion that I could do that with plain old Python too.</p>
<p>After a bit of training:</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">tf.reset_default_graph()</span><br><span class="line">dqn_runner = run_experiment.create_runner(DQN_PATH, schedule=<span class="string">'continuous_train'</span>)</span><br><span class="line">dqn_runner.run_experiment()</span><br></pre></td></tr></table></figure>
<p>Then we can run some similar code to before to generate a nice video:</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">rec = VideoRecorder(dqn_runner._environment.environment)</span><br><span class="line">action = dqn_runner._initialize_episode()</span><br><span class="line">rec.capture_frame()</span><br><span class="line"><span class="keyword">while</span> <span class="literal">True</span>:</span><br><span class="line">    observation, reward, is_terminal = dqn_runner._run_one_step(action)</span><br><span class="line">    rec.capture_frame()</span><br><span class="line">    <span class="keyword">if</span> is_terminal:</span><br><span class="line">      <span class="keyword">break</span> <span class="comment"># If the pole has fallen, quit.</span></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">      action = dqn_runner._agent.step(reward, observation)</span><br><span class="line">dqn_runner._end_episode(reward)</span><br><span class="line">rec.close()</span><br></pre></td></tr></table></figure>
<h2 id="RLLib-via-ray-project"><a href="#RLLib-via-ray-project" class="headerlink" title="RLLib via ray-project"></a><a href="https://ray.readthedocs.io/en/latest/rllib.html" target="_blank" rel="noopener">RLLib</a> via <a href="https://github.com/ray-project/ray" target="_blank" rel="noopener">ray-project</a></h2><p><img src="https://img.shields.io/github/stars/ray-project/ray.svg?style=flat-square" alt><br><img src="https://img.shields.io/github/issues-closed-raw/ray-project/ray.svg?style=flat-square" alt><br><img src="https://img.shields.io/github/contributors/ray-project/ray.svg?style=flat-square" alt><br><img src="https://img.shields.io/github/last-commit/ray-project/ray.svg?style=flat-square" alt><br><img src="https://img.shields.io/github/commit-activity/y/ray-project/ray.svg?style=flat-square" alt></p>
<p>Ray <a href="https://github.com/ray-project/ray/tree/ray-0.3.0" target="_blank" rel="noopener">started life</a> as a project that aimed to help Python users build scalable software, primarily for ML purposes. Since then it has added several modules that are dedicated to specific ML use cases. One is <a href="https://ray.readthedocs.io/en/latest/tune.html" target="_blank" rel="noopener">distributed hyperparameter tuning</a> and the other is <a href="https://ray.readthedocs.io/en/latest/rllib.html" target="_blank" rel="noopener">distributed RL</a>.</p>
<p>The consequence of this generalisation is that the popularity numbers are probably more due to the hyperparameter and general purpose scalability use case, rather than RL. Also, the distributed focus of the library means that the agent implementations tend to be those that are inherently distributed (e.g. A3C) or are attempting to solve problems that are so complex they need distributing so that they don’t take years to converge (e.g. Rainbow).</p>
<p>Despite this, if you are looking to productionise RL, or if you are repeating training many times for hyperparameter tuning or environment improvements, then it probably makes sense to use ray to be able to scale up and reduce feedback times. In fact, a number of other frameworks (specifically: <a href="##slm-lab">SLM-Lab</a> and <a href="##rlgraph">RLgraph</a>) actually use ray under the hood for this purpose.</p>
<p>I believe there is a strong applicability to RL here. The clear focus on distributed computation is good.The sheer number of commits and contributors is also reassuring. But there is a lot of the underlying code in C++. Some even in Java. Only 60% is python.</p>
<p>Despite this there is a very clear abstraction for <a href="https://github.com/ray-project/ray/blob/master/python/ray/rllib/policy/policy.py" target="_blank" rel="noopener"><code>Policy</code>s</a>, a nice, almost functional interface for agents called <a href="https://github.com/ray-project/ray/blob/master/python/ray/rllib/agents/trainer.py" target="_blank" rel="noopener"><code>Trainer</code>s</a> (see the <a href="https://github.com/ray-project/ray/blob/master/python/ray/rllib/agents/dqn/dqn.py" target="_blank" rel="noopener">DQN implementation</a> for an example of its usage), a <a href="https://github.com/ray-project/ray/blob/master/python/ray/rllib/models/model.py" target="_blank" rel="noopener"><code>Model</code></a> abstraction that allows the use of PyTorch or Tensorflow (yay!) and a few more for evaluation and policy optimisation.</p>
<p>Overall the documentation is excellent and clear architectural drawings are presented (see <a href="https://ray.readthedocs.io/en/latest/rllib-models.html" target="_blank" rel="noopener">this example</a>, for example). It is modular, scales well and is very well supported and accepted by the community. The only downside is the complexity of it all. That’s the price you pay for all this functionality.</p>
<h3 id="Getting-Started-2"><a href="#Getting-Started-2" class="headerlink" title="Getting Started"></a>Getting Started</h3><p>There is an <a href="https://github.com/ray-project/ray/issues/5033" target="_blank" rel="noopener">issue</a> with the version of pyarrow preinstalled in Google colab that isn’t compatible with ray. You have to uninstall the preinstalled version and restart the runtime, then it works.</p>
<p>I also couldn’t get the video rendering working in the same way we made the previous examples work. My hypothesis is that because they are running in separate processes they don’t have access to the fake <code>pyvirtualdisplay</code> device.</p>
<p>So despite this, let us try an example:</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">!pip uninstall -y pyarrow</span><br><span class="line">!pip install tensorflow ray[rllib] &gt; /dev/null 2&gt;&amp;1</span><br></pre></td></tr></table></figure>
<p>After you remove pyarrow and install rllib, you must restart the Notebook kernel. Next, import ray:</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> ray</span><br><span class="line"><span class="keyword">from</span> ray <span class="keyword">import</span> tune</span><br><span class="line"></span><br><span class="line">ray.init()</span><br></pre></td></tr></table></figure>
<p>And run a hyperparameter tuning job for the Cartpole environment using a DQN:</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">tune.run(</span><br><span class="line">    <span class="string">"DQN"</span>,</span><br><span class="line">    stop=&#123;<span class="string">"episode_reward_mean"</span>: <span class="number">100</span>&#125;,</span><br><span class="line">    config=&#123;</span><br><span class="line">        <span class="string">"env"</span>: <span class="string">"CartPole-v0"</span>,</span><br><span class="line">        <span class="string">"num_gpus"</span>: <span class="number">0</span>,</span><br><span class="line">        <span class="string">"num_workers"</span>: <span class="number">1</span>,</span><br><span class="line">        <span class="string">"lr"</span>: tune.grid_search([<span class="number">0.01</span>, <span class="number">0.001</span>, <span class="number">0.0001</span>]),</span><br><span class="line">        <span class="string">"monitor"</span>: <span class="literal">False</span>,</span><br><span class="line">    &#125;,</span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<p>There is a lot of syntactic sugar here, but it looks <a href="https://github.com/ray-project/ray/blob/master/python/ray/rllib/examples/custom_train_fn.py" target="_blank" rel="noopener">reasonably straightforward</a> to customise the training functionality (<a href="https://ray.readthedocs.io/en/latest/rllib-training.html#custom-training-workflows" target="_blank" rel="noopener">docs</a>).</p>
<h2 id="Keras-RL"><a href="#Keras-RL" class="headerlink" title="Keras-RL"></a><a href="https://github.com/keras-rl/keras-rl" target="_blank" rel="noopener">Keras-RL</a></h2><p><img src="https://img.shields.io/github/stars/keras-rl/keras-rl.svg?style=flat-square" alt><br><img src="https://img.shields.io/github/issues-closed-raw/keras-rl/keras-rl.svg?style=flat-square" alt><br><img src="https://img.shields.io/github/contributors/keras-rl/keras-rl.svg?style=flat-square" alt><br><img src="https://img.shields.io/github/last-commit/keras-rl/keras-rl.svg?style=flat-square" alt><br><img src="https://img.shields.io/github/commit-activity/y/keras-rl/keras-rl.svg?style=flat-square" alt></p>
<p>I love Keras. I love the abstraction, the simplicity, the anti-lock-in. When you look at the code below you can see the Keras magic. So you would think that keras-rl would be a perfect fit. However it doesn’t seem to have obtained as much traction as the other frameworks. If you <a href="https://keras-rl.readthedocs.io/en/latest/" target="_blank" rel="noopener">look at the documentation</a>, it’s empty. When you look at the commits there only a few brave souls that have done most of the work. Compare this to the <a href="https://github.com/keras-team/keras" target="_blank" rel="noopener">main Keras project</a>.</p>
<p>And I think I might know why. Keras was built from the ground up to allow users to quickly prototype different DL structures. This relied on the fact that the Neural Network primitives could be abstracted and modular. But when you look at the code for keras-rl, it’s implemented like it is in the textbooks. Each agent has it’s own implementation despite the similarities between SARSA and DQN, for example. Think of all the “tricks” that could be modularised, tricks like those that are used for Rainbow, which could allow people to experiment using these tricks in other agents. There is some level of modularity, but I think it is at a level that is too high.</p>
<p>But maybe it’s not too late, because there is so much promise here. If there were enough people interested, or maybe if there was more support from the core Keras project, then maybe this could be the go-to RL framework of the future. But for now, I don’t think it is. It’s almost as easy to obtain the benefits of Keras by using other frameworks that we have already discussed.</p>
<h3 id="Getting-Started-3"><a href="#Getting-Started-3" class="headerlink" title="Getting Started"></a>Getting Started</h3><p>The examples worked out of the box here and the only modifications I made were to use a mock Display and add some video recording of the tests. You can see that most of the code here is standard Keras code. The additions by Keras-RL aren’t really Keras related at all.</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> gym</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> keras.models <span class="keyword">import</span> Sequential</span><br><span class="line"><span class="keyword">from</span> keras.layers <span class="keyword">import</span> Dense, Activation, Flatten</span><br><span class="line"><span class="keyword">from</span> keras.optimizers <span class="keyword">import</span> Adam</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> rl.agents.dqn <span class="keyword">import</span> DQNAgent</span><br><span class="line"><span class="keyword">from</span> rl.policy <span class="keyword">import</span> BoltzmannQPolicy</span><br><span class="line"><span class="keyword">from</span> rl.memory <span class="keyword">import</span> SequentialMemory</span><br><span class="line"></span><br><span class="line">ENV_NAME = <span class="string">'CartPole-v0'</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Get the environment and extract the number of actions.</span></span><br><span class="line">env = gym.make(ENV_NAME)</span><br><span class="line">np.random.seed(<span class="number">123</span>)</span><br><span class="line">env.seed(<span class="number">123</span>)</span><br><span class="line">nb_actions = env.action_space.n</span><br><span class="line"></span><br><span class="line"><span class="comment"># Next, we build a very simple model.</span></span><br><span class="line">model = Sequential()</span><br><span class="line">model.add(Flatten(input_shape=(<span class="number">1</span>,) + env.observation_space.shape))</span><br><span class="line">model.add(Dense(<span class="number">16</span>))</span><br><span class="line">model.add(Activation(<span class="string">'relu'</span>))</span><br><span class="line">model.add(Dense(<span class="number">16</span>))</span><br><span class="line">model.add(Activation(<span class="string">'relu'</span>))</span><br><span class="line">model.add(Dense(<span class="number">16</span>))</span><br><span class="line">model.add(Activation(<span class="string">'relu'</span>))</span><br><span class="line">model.add(Dense(nb_actions))</span><br><span class="line">model.add(Activation(<span class="string">'linear'</span>))</span><br><span class="line">print(model.summary())</span><br><span class="line"></span><br><span class="line"><span class="comment"># Finally, we configure and compile our agent. You can use every built-in Keras optimizer and</span></span><br><span class="line"><span class="comment"># even the metrics!</span></span><br><span class="line">memory = SequentialMemory(limit=<span class="number">5000</span>, window_length=<span class="number">1</span>)</span><br><span class="line">policy = BoltzmannQPolicy()</span><br><span class="line">dqn = DQNAgent(model=model, nb_actions=nb_actions, memory=memory, nb_steps_warmup=<span class="number">10</span>,</span><br><span class="line">               target_model_update=<span class="number">1e-2</span>, policy=policy)</span><br><span class="line">dqn.compile(Adam(lr=<span class="number">1e-3</span>), metrics=[<span class="string">'mae'</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># Okay, now it's time to learn something! We visualize the training here for show, but this</span></span><br><span class="line"><span class="comment"># slows down training quite a lot. You can always safely abort the training prematurely using</span></span><br><span class="line"><span class="comment"># Ctrl + C.</span></span><br><span class="line">dqn.fit(env, nb_steps=<span class="number">2500</span>, visualize=<span class="literal">True</span>, verbose=<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># After training is done, we save the final weights.</span></span><br><span class="line">dqn.save_weights(<span class="string">'dqn_&#123;&#125;_weights.h5f'</span>.format(ENV_NAME), overwrite=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Finally, evaluate our algorithm for 5 episodes.</span></span><br><span class="line">dqn.test(Monitor(env, <span class="string">'.'</span>), nb_episodes=<span class="number">5</span>, visualize=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure>
<h2 id="TRFL"><a href="#TRFL" class="headerlink" title="TRFL"></a><a href="https://github.com/deepmind/trfl" target="_blank" rel="noopener">TRFL</a></h2><p><img src="https://img.shields.io/github/stars/deepmind/trfl.svg?style=flat-square" alt><br><img src="https://img.shields.io/github/issues-closed-raw/deepmind/trfl.svg?style=flat-square" alt><br><img src="https://img.shields.io/github/contributors/deepmind/trfl.svg?style=flat-square" alt><br><img src="https://img.shields.io/github/last-commit/deepmind/trfl.svg?style=flat-square" alt><br><img src="https://img.shields.io/github/commit-activity/y/deepmind/trfl.svg?style=flat-square" alt></p>
<p>TRFL is an opinionated extension to Tensorflow by Deepmind (so NOGP then ;-) ). Given the credentials you would have expected it to be popular, but the first thing you notice is the distinct lack of commits. Then the distinct <a href="https://github.com/deepmind/trfl/issues/17" target="_blank" rel="noopener">lack of examples and Tensorflow 2.0 support</a>.</p>
<p>The main issue is that it is too low-level. It’s the exact opposite of Keras-RL. The functionality that TRFL provides is a few helper functions, a <a href="https://github.com/deepmind/trfl/blob/master/trfl/action_value_ops.py#L40" target="_blank" rel="noopener">q-learning value function</a> for example, which takes in a load of Tensorflow Tensors with abstract names.</p>
<h3 id="Getting-Started-4"><a href="#Getting-Started-4" class="headerlink" title="Getting Started"></a>Getting Started</h3><p>I recommend taking a quick look at <a href="https://colab.research.google.com/drive/1r_SGbDBzEaKeijJFExgPTOcaglZcD0-S#scrollTo=627LbtjyZmYX" target="_blank" rel="noopener">this Notebook</a> as an example. But note that the code is very low level.</p>
<h2 id="Tensorforce"><a href="#Tensorforce" class="headerlink" title="Tensorforce"></a><a href="https://github.com/tensorforce/tensorforce" target="_blank" rel="noopener">Tensorforce</a></h2><p><img src="https://img.shields.io/github/stars/tensorforce/tensorforce.svg?style=flat-square" alt><br><img src="https://img.shields.io/github/issues-closed-raw/tensorforce/tensorforce.svg?style=flat-square" alt><br><img src="https://img.shields.io/github/contributors/tensorforce/tensorforce.svg?style=flat-square" alt><br><img src="https://img.shields.io/github/last-commit/tensorforce/tensorforce.svg?style=flat-square" alt><br><img src="https://img.shields.io/github/commit-activity/y/tensorforce/tensorforce.svg?style=flat-square" alt></p>
<p>Tensorforce has similar aims to <a href="##TRFL">TRFL</a>. It attempts to abstract RL primitives whilst targeting Tensorflow. By using Tensorflow then you gain all of the benefits of using Tensorflow, i.e. graph models, easier, cross-platform deployment.</p>
<p>There are four high-level abstractions of an <a href="https://github.com/tensorforce/tensorforce/blob/master/tensorforce/environments/environment.py" target="_blank" rel="noopener"><code>Environment</code></a>, <a href="https://github.com/tensorforce/tensorforce/blob/master/tensorforce/execution/runner.py" target="_blank" rel="noopener"><code>Runner</code></a>, <a href="https://github.com/tensorforce/tensorforce/blob/master/tensorforce/agents/agent.py" target="_blank" rel="noopener"><code>Agent</code></a> and <a href="https://github.com/tensorforce/tensorforce/blob/master/tensorforce/models/model.py" target="_blank" rel="noopener"><code>Model</code></a>. These mostly do what you would expect, but the “Model” abstraction is not something you would normally see. A <code>Model</code> sits within an <code>Agent</code> and defines the policy of the agent. This is nice because, for example, the standard <a href="https://github.com/tensorforce/tensorforce/blob/master/tensorforce/models/q_model.py" target="_blank" rel="noopener">Q-learning model</a> can be overridden by the <a href="https://github.com/tensorforce/tensorforce/blob/master/tensorforce/models/q_nstep_model.py" target="_blank" rel="noopener">Q-learning n-step model</a>, only changing one small function. This is precisely the middle ground between TRFL and Keras that I was looking for. And it’s implemented in an OOP way, which some people will like, others wont. But at least the abstraction is there.</p>
<p>The downside of libraries like this, or any DL focused RL library, is that much of the code is complicated by the underlying DL framework. The same is true here. For example, the <a href="https://github.com/tensorforce/tensorforce/blob/major-revision/tensorforce/core/models/random_model.py" target="_blank" rel="noopener">random model</a>, i.e. one that chooses a random action, something that should take precisely one line of code, is 79 lines long. I’m being a little facetious here (license, class boilerplate, newlines, etc.) but hopefully you understand my point.</p>
<p>It also means that there is no implementation of “simple” RL algorithms, i.e. those that don’t use models. E.g. entropy, bandits, simple MDPs, SARSA, some tabular methods, etc. And the reason for this is that you don’t need a DL framework for these models.</p>
<p>So in summary, I think the level of abstraction is spot on. But the benefits/problems of limiting yourself to a DL framework remain.</p>
<p>Note that this is based upon version <code>0.4.3</code> and a major rewrite is underway.</p>
<h3 id="Getting-Started-5"><a href="#Getting-Started-5" class="headerlink" title="Getting Started"></a>Getting Started</h3><p>The getting started example is sensible. We’re creating an environment, an agent and a runner (the thing that actually does the training). The specifications for the agent is a little different though. It reminds me of the <a href="##google\ dopamine">Dopamine</a> Gin config, except it’s using standard json. In the example I’m getting those specifications from their examples directory, but you can imagine how easy it would be to run a hyperparameter search using them.</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line">environment = OpenAIGym(</span><br><span class="line">    gym_id=<span class="string">"CartPole-v0"</span>,</span><br><span class="line">    monitor=<span class="string">"."</span>,</span><br><span class="line">    monitor_safe=<span class="literal">False</span>,</span><br><span class="line">    monitor_video=<span class="number">10</span>,</span><br><span class="line">    visualize=<span class="literal">True</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> urllib.request.urlopen(<span class="string">"https://raw.githubusercontent.com/tensorforce/tensorforce/master/examples/configs/dqn.json"</span>) <span class="keyword">as</span> url:</span><br><span class="line">  agent = json.loads(url.read().decode())</span><br><span class="line">  print(agent)</span><br><span class="line"><span class="keyword">with</span> urllib.request.urlopen(<span class="string">"https://raw.githubusercontent.com/tensorforce/tensorforce/master/examples/configs/mlp2_network.json"</span>) <span class="keyword">as</span> url:</span><br><span class="line">  network = json.loads(url.read().decode())</span><br><span class="line">  print(network)</span><br><span class="line"></span><br><span class="line">agent = Agent.from_spec(</span><br><span class="line">  spec=agent,</span><br><span class="line">  kwargs=dict(</span><br><span class="line">    states=environment.states,</span><br><span class="line">    actions=environment.actions,</span><br><span class="line">    network=network</span><br><span class="line">  )</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">runner = Runner(</span><br><span class="line">    agent=agent,</span><br><span class="line">    environment=environment,</span><br><span class="line">    repeat_actions=<span class="number">1</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">runner.run(</span><br><span class="line">    num_timesteps=<span class="number">200</span>,</span><br><span class="line">    num_episodes=<span class="number">200</span>,</span><br><span class="line">    max_episode_timesteps=<span class="number">200</span>,</span><br><span class="line">    deterministic=<span class="literal">True</span>,</span><br><span class="line">    testing=<span class="literal">False</span>,</span><br><span class="line">    sleep=<span class="literal">None</span></span><br><span class="line">)</span><br><span class="line">runner.close()</span><br></pre></td></tr></table></figure>
<h2 id="Facebook-Horizon"><a href="#Facebook-Horizon" class="headerlink" title="Facebook Horizon"></a><a href="https://github.com/facebookresearch/Horizon" target="_blank" rel="noopener">Facebook Horizon</a></h2><p><img src="https://img.shields.io/github/stars/facebookresearch/Horizon.svg?style=flat-square" alt><br><img src="https://img.shields.io/github/issues-closed-raw/facebookresearch/Horizon.svg?style=flat-square" alt><br><img src="https://img.shields.io/github/contributors/facebookresearch/Horizon.svg?style=flat-square" alt><br><img src="https://img.shields.io/github/last-commit/facebookresearch/Horizon.svg?style=flat-square" alt><br><img src="https://img.shields.io/github/commit-activity/y/facebookresearch/Horizon.svg?style=flat-square" alt></p>
<p>Horizon is a framework from Facebook that is dominated by PyTorch. Another DL focused library then. Also:</p>
<blockquote>
<p>The main use case of Horizon is to train RL models in the batch setting. &hellip; Specifically, we try to learn the best possible policy given the input data.</p>
</blockquote>
<p>So like other frameworks the focus is off-policy, model driven RL with DL in the model. But this is differentiated due to the use of PyTorch. You could also compare this to Keras-RL using PyTorch as the backend for Keras.</p>
<p>I’ve already discussed the cons of such a focused framework in the <a href="##tensorforce">Tensorforce</a> section, so I won’t state them again.</p>
<p>There are several interesting differences though. There is no tight Gym integration. Instead they intentionally decouple the two by dumping Gym data into JSON, then reading the JSON back into the agent. This might sound verbose, but is actually really good for <a href="https://en.wikipedia.org/wiki/Loose_coupling" target="_blank" rel="noopener">decoupling</a> and therefore more scalable, less fragile and more flexible. The downside is that there are more hoops to jump through due to the increased complexity.</p>
<p>However, as inconceivable as it sounds, there is no pip installer for Horizon. You have to use conda, install onnx, install java, setup the <code>JAVA_HOME</code> to point to conda, install Spark, install Gym (fair enough), install Apache thrift and then build Horizon. Wow. (Bonus points if you counted how many steps that was).</p>
<p>So I think it suffice to say that I’m not going to attempt to install this in the demo Notebook.</p>
<h3 id="Getting-Started-6"><a href="#Getting-Started-6" class="headerlink" title="Getting Started"></a>Getting Started</h3><p>You’ll need a lot of time and a lot of patience. Follow <a href="https://horizonrl.com/installation.html" target="_blank" rel="noopener">the build instructions</a>, then follow <a href="https://horizonrl.com/usage.html#offline-rl-training-batch-rl" target="_blank" rel="noopener">the training guide</a>. I can’t vouch for it because I have a life to get on with.</p>
<h2 id="Nervana-Systems-Coach"><a href="#Nervana-Systems-Coach" class="headerlink" title="Nervana Systems Coach"></a><a href="https://github.com/NervanaSystems/coach" target="_blank" rel="noopener">Nervana Systems Coach</a></h2><p><img src="https://img.shields.io/github/stars/NervanaSystems/coach.svg?style=flat-square" alt><br><img src="https://img.shields.io/github/issues-closed-raw/NervanaSystems/coach.svg?style=flat-square" alt><br><img src="https://img.shields.io/github/contributors/NervanaSystems/coach.svg?style=flat-square" alt><br><img src="https://img.shields.io/github/last-commit/NervanaSystems/coach.svg?style=flat-square" alt><br><img src="https://img.shields.io/github/commit-activity/y/NervanaSystems/coach.svg?style=flat-square" alt></p>
<p>The first thing you will notice when you look at this framework is the <a href="https://github.com/NervanaSystems/coach#supported-algorithms" target="_blank" rel="noopener">number of implemented algorithms</a>. It is colossal and must have taken several people many weeks to implement. The second thing you will notice is the <a href="https://github.com/NervanaSystems/coach#supported-environments" target="_blank" rel="noopener">number of integrated Environments</a>. Considering how much time this must have taken, it gives a lot of hope for the rest of the framework.</p>
<p>It comes with a <a href="https://nervanasystems.github.io/coach/dashboard.html" target="_blank" rel="noopener">dedicated dashboard</a> which looks pretty nice. Most of the other frameworks rely on the Tensorboard project.</p>
<p>One particular wow feature that I haven’t seen before, is <a href="https://nervanasystems.github.io/coach/components/orchestrators/index.html" target="_blank" rel="noopener">inbuilt deployment for Kubernetes</a>. I think that the orchestration of Coach by Coach is a step too far, but the fact that they’ve even thought about it means that it is probably scalable enough to deploy onto Kuberentes with standard tooling.</p>
<p>The level of modularity is astounding. For example there are classes that implement all sorts of <a href="https://nervanasystems.github.io/coach/components/exploration_policies/index.html" target="_blank" rel="noopener">exploration strategies</a> and allow you to make all sorts of changes to the <a href="https://nervanasystems.github.io/coach/design/network.html" target="_blank" rel="noopener">various model designs</a>.</p>
<p>The ONLY thing that I can think of that is a little annoying is the same limitation of forcing me to use DL as the model. I remain convinced that a subset of simpler applications do not require anything nearly as complex as DL and could benefit for more traditional Regression methods. However, I’m sure it should be reasonably easy to add in a little stub class that removes the DL stuff.</p>
<p>Interestingly, the framework <a href="https://nervanasystems.github.io/coach/usage.html#switching-between-deep-learning-frameworks" target="_blank" rel="noopener">supports Tensorflow and MXNet</a> due to it’s use of Keras. This means that PyTorch is not supported, because Keras doesn’t support PyTorch.</p>
<p>Frankly, I can’t understand why this framework is so unpopular in any way of measuring it. In terms of stars. In terms of the <a href="https://www.google.com/search?q=" title="nervana+systems+coach" target="_blank" rel="noopener">number of google pages</a> (the answer is 7, if you are wondering). Compare that to <a href="https://www.google.com/search?q=" title="google+dopamine" target="_blank" rel="noopener">Google Dopamine</a> for example, with 16500 pages.</p>
<p>It is certainly the most comprehensive framework with the best documentation and a fantastic level of modularity. They’ve even got a ❤️ <a href="https://github.com/NervanaSystems/coach/blob/v0.12.1/tutorials/0.%20Quick%20Start%20Guide.ipynb" target="_blank" rel="noopener">Getting Started Notebook</a> ❤️.</p>
<h2 id="Getting-Started-7"><a href="#Getting-Started-7" class="headerlink" title="Getting Started"></a>Getting Started</h2><p>There are two important notes I’d like to point out. First, make sure you are looking at a tagged version of the documentation or the demos. There are some new features in the master branch that don’t work with a pip installed version. Second, it depends on OpenAI Gym version <code>0.12.5</code>, which isn’t installed in colab. You’ll need to run <code>!pip install gym==0.12.5</code> and restart the runtime.</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line">tf.reset_default_graph() <span class="comment"># So that we don't get an error for TF when we re-run</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> rl_coach.agents.clipped_ppo_agent <span class="keyword">import</span> ClippedPPOAgentParameters</span><br><span class="line"><span class="keyword">from</span> rl_coach.environments.gym_environment <span class="keyword">import</span> GymVectorEnvironment</span><br><span class="line"><span class="keyword">from</span> rl_coach.graph_managers.basic_rl_graph_manager <span class="keyword">import</span> BasicRLGraphManager</span><br><span class="line"><span class="keyword">from</span> rl_coach.graph_managers.graph_manager <span class="keyword">import</span> ScheduleParameters</span><br><span class="line"><span class="keyword">from</span> rl_coach.core_types <span class="keyword">import</span> TrainingSteps, EnvironmentEpisodes, EnvironmentSteps</span><br><span class="line"><span class="keyword">from</span> rl_coach.base_parameters <span class="keyword">import</span> VisualizationParameters</span><br><span class="line"><span class="keyword">global</span> experiment_path; experiment_path = <span class="string">'.'</span> <span class="comment"># Because of some bizzare global in the mp4 dumping code</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Custom schedule to speed up training. We don't really care about the results.</span></span><br><span class="line">schedule_params = ScheduleParameters()</span><br><span class="line">schedule_params.improve_steps = TrainingSteps(<span class="number">200</span>)</span><br><span class="line">schedule_params.steps_between_evaluation_periods = EnvironmentSteps(<span class="number">200</span>)</span><br><span class="line">schedule_params.evaluation_steps = EnvironmentEpisodes(<span class="number">10</span>)</span><br><span class="line">schedule_params.heatup_steps = EnvironmentSteps(<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">graph_manager = BasicRLGraphManager(</span><br><span class="line">    agent_params=ClippedPPOAgentParameters(),</span><br><span class="line">    env_params=GymVectorEnvironment(level=<span class="string">'CartPole-v0'</span>),</span><br><span class="line">    schedule_params=schedule_params,</span><br><span class="line">    vis_params=VisualizationParameters(dump_mp4=<span class="literal">True</span>) <span class="comment"># So we can dump the video</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<h2 id="MAgent"><a href="#MAgent" class="headerlink" title="MAgent"></a><a href="https://github.com/geek-ai/MAgent" target="_blank" rel="noopener">MAgent</a></h2><p><img src="https://img.shields.io/github/stars/geek-ai/MAgent.svg?style=flat-square" alt><br><img src="https://img.shields.io/github/issues-closed-raw/geek-ai/MAgent.svg?style=flat-square" alt><br><img src="https://img.shields.io/github/contributors/geek-ai/MAgent.svg?style=flat-square" alt><br><img src="https://img.shields.io/github/last-commit/geek-ai/MAgent.svg?style=flat-square" alt><br><img src="https://img.shields.io/github/commit-activity/y/geek-ai/MAgent.svg?style=flat-square" alt></p>
<p>MAgent is a framework that allows you to solve many-agent RL problems. This is a completely different aim compared to all the other “traditional” RL frameworks that use only a single or very few agents. They claim it can scale up to millions of agents.</p>
<p>But again, no pip installer. Please, everyone, create pip installers for you projects. It’s vitally important for ease of use and therefore project traction. I guess it’s because the whole project is written in C, presumably for performance reasons.</p>
<p>It’s using Tensorflow under the hood and builds its own gridworld-like enironment. The Agents are designed with “real-life” simulations in mind. For example you can specify the size of the agent, how far it can see; things like that. The observations that are passed to the Agents are grids. The actions that they can take are limited to moving, attacking and turning. They are rewarded according to a flexible rule definition.</p>
<p>In short, the framework is setup to handle game of life style games out of the box, with some extra modularity on how the agents behave and are rewarded. Due to this we can use some of the more advanced DL methods to train the agents to perform complex, coordinated tasks. Like surround prey so that it can’t move. You can learn more in the <a href="https://github.com/geek-ai/MAgent/blob/master/doc/get_started.md" target="_blank" rel="noopener">getting started guide</a>.</p>
<p>I am very impressed with the idea. But as you can see from the Github stats above, 4 commits a year basically means that it is hardly being used. The last major updates were in 2017. This is a shame, because it represents something very different compared to the other frameworks. It would be great if someone could make it easier to use, or replicate the framework in idiomatic Python, so that it becomes easier to use.</p>
<h3 id="Getting-Started-8"><a href="#Getting-Started-8" class="headerlink" title="Getting Started"></a>Getting Started</h3><p>So I almost got it working in the Notebook. I tried a few examples from the <a href="https://github.com/geek-ai/MAgent/blob/master/doc/get_started.md" target="_blank" rel="noopener">getting started guide</a>. The training version takes hours, so I bailed on that quite quickly. The <code>examples/api_demo.py</code> however is just testing the learnt models, so that is lightning quick.</p>
<p>However, it renders the environment in some proprietary text format. You need to run a random webserver binary that parses and hosts the render in the browser. Because we’re in colab, it doesn’t allow you to run a webserver. So I tried downloading the files, but we built the binaries on colab, not on a mac, so I wasn’t able to run the binary.</p>
<p>So that was a bit frustrating. It would have been much simpler if it had just rendered it in some standard format like mp4 of a gif or something. And also disappointing, because I was looking forward to generating some complex behavior.</p>
<p>But just so you are not disappointed, here is some eye candy from the author. Please excuse the <a href="www.youtube.com/embed/HCSm0kVolqI">audio</a>!</p>
<p>And here are the remains of the code that worked:</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">!git <span class="built_in">clone</span> https://github.com/geek-ai/MAgent.git</span><br><span class="line">!sudo apt-get install cmake libboost-system-dev libjsoncpp-dev libwebsocketpp-dev</span><br><span class="line">%<span class="built_in">cd</span> MAgent</span><br><span class="line">!bash build.sh</span><br><span class="line"></span><br><span class="line">!PYTHONPATH=$(<span class="built_in">pwd</span>)/python:<span class="variable">$PYTHONPATH</span> python examples/api_demo.py</span><br></pre></td></tr></table></figure>
<p>You can exchange that last call to any of the python files in teh examples folder.</p>
<h2 id="TF-Agents"><a href="#TF-Agents" class="headerlink" title="TF-Agents"></a><a href="https://github.com/tensorflow/agents" target="_blank" rel="noopener">TF-Agents</a></h2><p><img src="https://img.shields.io/github/stars/tensorflow/agents.svg?style=flat-square" alt><br><img src="https://img.shields.io/github/issues-closed-raw/tensorflow/agents.svg?style=flat-square" alt><br><img src="https://img.shields.io/github/contributors/tensorflow/agents.svg?style=flat-square" alt><br><img src="https://img.shields.io/github/last-commit/tensorflow/agents.svg?style=flat-square" alt><br><img src="https://img.shields.io/github/commit-activity/y/tensorflow/agents.svg?style=flat-square" alt></p>
<p>Tensorflow-Agents (TF-Agents) is another <a href="##google\ dopamine">NOGP</a> from Google with the focus squarely on Tensorflow. So treat this as direct competition to <a href="##trfl">TRFL</a>, <a href="##tensorforce">Tensorforce</a> and <a href="##google\ dopamine">Dopamine</a>.</p>
<p>Which begs the question: why have more Google employees created another Tensorflow-abstraction-for-RL when TRFL and Dopamine already exist? In an <a href="https://github.com/tensorflow/agents/issues/15" target="_blank" rel="noopener">issue discussing the relationship between TF-Agents and Dopamine</a> the contributors suggest that:</p>
<blockquote>
<p>it seems that Dopamine and TF-Agent strongly overlap. Although dopamine aims at being used for fast prototyping and benchmarking as the reproducibility has been put at the core of the project, whereas TF-Agent would be more used for production-level reinforcement learning algorithms.</p>
</blockquote>
<p>To be honest, I’m not sure what “production-level” means at this point. There are some <a href="https://github.com/tensorflow/agents/tree/master/tf_agents/colabs" target="_blank" rel="noopener">great colab examples</a>, but there is no documentation. And you certainly shouldn’t be using Notebooks in production.</p>
<p>Once you start digging into the examples, then it becomes clear that the code is very Tensorflow heavy. For example, the <a href="https://github.com/tensorflow/agents/blob/master/tf_agents/colabs/1_dqn_tutorial.ipynb" target="_blank" rel="noopener">simple Cartpole example</a> has a lot of lines of code. Mainly because there is a lot of explanation and debuging code in there, but it looks like a sign of things to come.</p>
<p>I must admit though, the code does look very nice. It’s nicely separated and the modularity looks good. All the abstractions you would expect are there. The only thing I want to pick on is the <a href="https://github.com/tensorflow/agents/blob/master/tf_agents/agents/tf_agent.py" target="_blank" rel="noopener"><code>Agent</code> abstraction</a>. This is the base class and it is directly coupled to Tensorflow. It is a Tensorflow module. This adds a significant amount of complexity and I wish that it was abstracted away so I didn’t have to worry about it until I need it. The same is true for the vast majority of other abstractions; they are all Tensorflow modules.</p>
<p>With that said, it is clear that this is a far more serious and capable library than <a href="##trfl">TRFL</a>.</p>
<h3 id="Getting-Started-9"><a href="#Getting-Started-9" class="headerlink" title="Getting Started"></a>Getting Started</h3><p>There is already an <a href="https://github.com/tensorflow/agents/tree/master/tf_agents/colabs" target="_blank" rel="noopener">extensive set of Notebooks available in their repository</a>, so I won’t waste time just copy and pasting here. You can also run them <a href="https://colab.research.google.com/github/tensorflow/agents/blob/master/tf_agents/colabs/1_dqn_tutorial.ipynb" target="_blank" rel="noopener">directly in colab too</a>.</p>
<p>The video below shows three episodes of the cartpole. To me it looks like it is having a fit. Constant nudges to rebalance.</p>
<h2 id="SLM-Lab"><a href="#SLM-Lab" class="headerlink" title="SLM-Lab"></a><a href="https://github.com/kengz/SLM-Lab" target="_blank" rel="noopener">SLM-Lab</a></h2><p><img src="https://img.shields.io/github/stars/kengz/SLM-Lab.svg?style=flat-square" alt><br><img src="https://img.shields.io/github/issues-closed-raw/kengz/SLM-Lab.svg?style=flat-square" alt><br><img src="https://img.shields.io/github/contributors/kengz/SLM-Lab.svg?style=flat-square" alt><br><img src="https://img.shields.io/github/last-commit/kengz/SLM-Lab.svg?style=flat-square" alt><br><img src="https://img.shields.io/github/commit-activity/y/kengz/SLM-Lab.svg?style=flat-square" alt></p>
<p>SLM-Lab is modular RL framework based upon PyTorch. It appears to be aimed more towards researchers. They stress the importance of modularity, but <a href="https://github.com/kengz/SLM-Lab#simplicity" target="_blank" rel="noopener">rightly state that</a> being simple and modular is probably not possible; it is a compromise between the two. Interestingly it also uses <a href="##RLLib\ via\ ray-project">the Ray project</a> under the hood to make it scalable.</p>
<p>Despite starting in 2017, the small number of contributors and the relative popularity in terms of github stars, there is lots of activity. The vast majority of the commits are one-liners, but the commitment by the authors is amazing.</p>
<p>Unfortunately this is another non-pip install framework and attempts to install a whole load of build related C libraries and miniconda. Which is problematic in colab. Being a good Engineer I ignored all the documentation and tried to get it working myself through trial and error. It almost worked, but I <a href="https://github.com/kengz/SLM-Lab/issues/373" target="_blank" rel="noopener">stumbled across a problem when initialising pytorch</a> that I didn’t know how to fix.</p>
<p>So unfortunately, again, you will have to settle for an example image from the authors.</p>
<p><img src="https://user-images.githubusercontent.com/8209263/49688875-87e56e80-facd-11e8-90be-9d6be7bace03.gif" alt></p>
<p>I struggled a bit with the documentation. The <a href="https://kengz.gitbooks.io/slm-lab/content/usage/aeb-design.html" target="_blank" rel="noopener">architecture documentation</a> is limited and the rest is focused towards usage. But by usage I mean <a href="https://kengz.gitbooks.io/slm-lab/content/usage/spec-file.html" target="_blank" rel="noopener">running experiments on current implementations</a>. I struggled to find the documentation that told me how to plug modules together in different ways. I presume they intend that this should be done via the JSON spec files. Indeed the original motivation was:</p>
<blockquote>
<p>There was a need for a framework that would allow us to compare algorithms and environments, quickly set up experiments to test hypotheses, reuse components, analyze and compare results, log results.</p>
</blockquote>
<p>So the goal here is to allow reuse via configuration, much like <a href="##google\ dopamine">Dopamine</a> and <a href="##tensorforce">Tensorforce</a>. This “RL as configuration” seems to be a theme! However, I’m not convinced. I would argue that code is more idiomatic, more flexible. It is what people are used to. Every time you do something via configuration that is another <em>Domain Specific Language</em> (DSL) that users have to learn. And because that DSL is generally static (Gin is not) then the implementation of the DSL sets the limitation. It is never going to suit everyone because there will be an edge case that isn’t covered by the DSL.</p>
<h2 id="DeeR"><a href="#DeeR" class="headerlink" title="DeeR"></a><a href="https://github.com/VinF/deer" target="_blank" rel="noopener">DeeR</a></h2><p><img src="https://img.shields.io/github/stars/VinF/deer.svg?style=flat-square" alt><br><img src="https://img.shields.io/github/issues-closed-raw/VinF/deer.svg?style=flat-square" alt><br><img src="https://img.shields.io/github/contributors/VinF/deer.svg?style=flat-square" alt><br><img src="https://img.shields.io/github/last-commit/VinF/deer.svg?style=flat-square" alt><br><img src="https://img.shields.io/github/commit-activity/y/VinF/deer.svg?style=flat-square" alt></p>
<p>The initial impression of DeeR is a good one. It has a pip installer. It waxes lyrical about modularity. It has only <strong>two</strong> Python dependencies; <code>numpy</code> and <code>Joblib</code>. So no nasty C make process to get it working, great!</p>
<p>The documentation is clear, but is missing some overall architecture documentation. You have to dig into the classes/code to find the documentation. But when you do it is good.</p>
<p>The “modules” are split mostly in the way you would expect. Modules for the <a href="https://deer.readthedocs.io/en/master/modules/environments.html" target="_blank" rel="noopener"><code>Environment</code></a>, <a href="https://deer.readthedocs.io/en/master/modules/agents.html" target="_blank" rel="noopener"><code>Agent</code></a>, and <a href="https://deer.readthedocs.io/en/master/modules/policies.html" target="_blank" rel="noopener"><code>Policies</code></a>.</p>
<p>There is an interesting class called <a href="https://deer.readthedocs.io/en/master/modules/controllers.html" target="_blank" rel="noopener"><code>Controller</code></a> which is not standard. This class provides lifecycle hooks that you can attach to; events like the end of an episode or whenever an action is taken. For example, if you wanted to do some logging at the end of an episode, then you could subclass this class and override the <a href="https://github.com/VinF/deer/blob/master/deer/experiment/base_controllers.py#L52" target="_blank" rel="noopener"><code>onEpisodeEnd</code></a>. There are several example controllers, one of which is an <a href="https://github.com/VinF/deer/blob/master/deer/experiment/base_controllers.py#L149" target="_blank" rel="noopener"><code>EpsilonController</code></a>. This allows you to dymaically alter the <code>eta</code> or <code>epsilon</code> value in e-greedy algorithms.</p>
<p>This is quite powerful as it allows you to alter the learning process in mid flight. But from a Software Engineering perspective this is quite risky. Any Functional Programmer will tell you not to mutate the state of another object because “Dragons be here”. It would have been nicer if the API was more functional and you could pass in functions that computed, for example, the agent’s next <code>eta</code>, rather than mutating the state of the agent directly. This may make it slightly more complex, though.</p>
<p>The framework also contains a <a href="https://deer.readthedocs.io/en/master/modules/learning-algorithms.html" target="_blank" rel="noopener">few learning algorithms</a> but it’s certainly not as comprehensive as something like <a href="##coach">Coach</a>.</p>
<h3 id="Getting-Started-10"><a href="#Getting-Started-10" class="headerlink" title="Getting Started"></a>Getting Started</h3><p>Thanks to the pip install and very few dependencies, this was probably the easiest framework to get up and running.</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">!pip install git+git://github.com/VINF/deer.git@master</span><br><span class="line">!git <span class="built_in">clone</span> https://github.com/VinF/deer.git</span><br></pre></td></tr></table></figure>
<p>I cloned the git repo so I could run the examples. Then it’s just a case of importing everything:</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">%cd /content/deer/examples/toy_env</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> deer.agent <span class="keyword">import</span> NeuralAgent</span><br><span class="line"><span class="keyword">from</span> deer.learning_algos.q_net_keras <span class="keyword">import</span> MyQNetwork</span><br><span class="line"><span class="keyword">from</span> Toy_env <span class="keyword">import</span> MyEnv <span class="keyword">as</span> Toy_env</span><br><span class="line"><span class="keyword">import</span> deer.experiment.base_controllers <span class="keyword">as</span> bc</span><br></pre></td></tr></table></figure>
<p>And stealing the example:</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line">rng = np.random.RandomState(<span class="number">123456</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># --- Instantiate environment ---</span></span><br><span class="line">env = Toy_env(rng)</span><br><span class="line"></span><br><span class="line"><span class="comment"># --- Instantiate qnetwork ---</span></span><br><span class="line">qnetwork = MyQNetwork(</span><br><span class="line">    environment=env,</span><br><span class="line">    random_state=rng)</span><br><span class="line"></span><br><span class="line"><span class="comment"># --- Instantiate agent ---</span></span><br><span class="line">agent = NeuralAgent(</span><br><span class="line">    env,</span><br><span class="line">    qnetwork,</span><br><span class="line">    random_state=rng)</span><br><span class="line"></span><br><span class="line"><span class="comment"># --- Bind controllers to the agent ---</span></span><br><span class="line"><span class="comment"># Before every training epoch, we want to print a summary of the agent's epsilon, discount and</span></span><br><span class="line"><span class="comment"># learning rate as well as the training epoch number.</span></span><br><span class="line">agent.attach(bc.VerboseController())</span><br><span class="line"></span><br><span class="line"><span class="comment"># During training epochs, we want to train the agent after every action it takes.</span></span><br><span class="line"><span class="comment"># Plus, we also want to display after each training episode (!= than after every training) the average bellman</span></span><br><span class="line"><span class="comment"># residual and the average of the V values obtained during the last episode.</span></span><br><span class="line">agent.attach(bc.TrainerController())</span><br><span class="line"></span><br><span class="line"><span class="comment"># All previous controllers control the agent during the epochs it goes through. However, we want to interleave a</span></span><br><span class="line"><span class="comment"># "test epoch" between each training epoch. We do not want these test epoch to interfere with the training of the</span></span><br><span class="line"><span class="comment"># agent. Therefore, we will disable these controllers for the whole duration of the test epochs interleaved this</span></span><br><span class="line"><span class="comment"># way, using the controllersToDisable argument of the InterleavedTestEpochController. The value of this argument</span></span><br><span class="line"><span class="comment"># is a list of the indexes of all controllers to disable, their index reflecting in which order they were added.</span></span><br><span class="line">agent.attach(bc.InterleavedTestEpochController(</span><br><span class="line">    epoch_length=<span class="number">500</span>,</span><br><span class="line">    controllers_to_disable=[<span class="number">0</span>, <span class="number">1</span>]))</span><br><span class="line"></span><br><span class="line"><span class="comment"># --- Run the experiment ---</span></span><br><span class="line">agent.run(n_epochs=<span class="number">100</span>, epoch_length=<span class="number">1000</span>)</span><br></pre></td></tr></table></figure>
<p>Here we are instantiating an environment, creating the Q-Learning algorithm and creating the agent that uses that algorithm. Next we use the <code>.attach()</code> function on the agent to appen all of these <code>Controller</code>s we have been talking about. They add logging and interleave training periods and testing periods.</p>
<p>If we wanted to edit any of this we just need to reimplement the piece that we’re interested in. Great!</p>
<p><img src="https://WinderResearch.com/a-comparison-of-reinforcement-learning-frameworks-dopamine-rllib-keras-rl-coach-trfl-tensorforce-coach-and-more/./deer.png" alt></p>
<p>The only issue was that the toy example didn’t work! I’m not sure why, but it’s probably something silly. The training values looked a little weird in that the test score was always 0, and the training loss increased over time. Maybe it got stuck. Not sure. I’m sure it’s something silly.</p>
<h2 id="Garage"><a href="#Garage" class="headerlink" title="Garage"></a><a href="https://github.com/rlworkgroup/garage" target="_blank" rel="noopener">Garage</a></h2><p><img src="https://img.shields.io/github/stars/rlworkgroup/garage.svg?style=flat-square" alt><br><img src="https://img.shields.io/github/issues-closed-raw/rlworkgroup/garage.svg?style=flat-square" alt><br><img src="https://img.shields.io/github/contributors/rlworkgroup/garage.svg?style=flat-square" alt><br><img src="https://img.shields.io/github/last-commit/rlworkgroup/garage.svg?style=flat-square" alt><br><img src="https://img.shields.io/github/commit-activity/y/rlworkgroup/garage.svg?style=flat-square" alt></p>
<p>Garage is a follow-on from <a href="https://github.com/rll/rllab" target="_blank" rel="noopener">rllab</a> with the same aims, but just community, rather than individual support. The <a href="https://rlgarage.readthedocs.io/en/latest/index.html" target="_blank" rel="noopener">documentation</a> is a little sparse. For instance, it doesn’t highlight that it implements a <a href="https://github.com/rlworkgroup/garage/tree/master/src/garage/tf/algos" target="_blank" rel="noopener">large number of algorithms</a>. And also a <a href="https://github.com/rlworkgroup/garage/tree/master/src/garage/tf/policies" target="_blank" rel="noopener">huge number of policies</a>. In fact, there’s pretty much everything you could ever need in this <a href="https://github.com/rlworkgroup/garage/tree/master/src/garage/tf" target="_blank" rel="noopener">quiet little directory</a>.</p>
<p>But it is very tightly coupled to Tensorflow, if that is a problem for you.</p>
<p>There’s so much functionality here, but it is completely hidden. The code is reasonably well documented, but it’s not exposed. You have to dig through it to find it.</p>
<p>Again there is no pip installer. Just some custom conda install and some <code>apt-get</code> dependencies.</p>
<p>So I can see that there is a huge amount of value in the algorithm implementations, but I’m going to skip the getting started this time.</p>
<h2 id="Surreal"><a href="#Surreal" class="headerlink" title="Surreal"></a><a href="https://github.com/SurrealAI/surreal" target="_blank" rel="noopener">Surreal</a></h2><p><img src="https://img.shields.io/github/stars/SurrealAI/surreal.svg?style=flat-square" alt><br><img src="https://img.shields.io/github/issues-closed-raw/SurrealAI/surreal.svg?style=flat-square" alt><br><img src="https://img.shields.io/github/contributors/SurrealAI/surreal.svg?style=flat-square" alt><br><img src="https://img.shields.io/github/last-commit/SurrealAI/surreal.svg?style=flat-square" alt><br><img src="https://img.shields.io/github/commit-activity/y/SurrealAI/surreal.svg?style=flat-square" alt></p>
<p>Surreal is a suite of applications. Foremost it is an RL framwork. But to make sure they don’t build just another framework, they also provide a new Robotics simulator, an orchestrator, a cloud infrastructure provisioner and a protocol for distributed computing. It comes from Stanford, hence academic in approach and use-case, hence the default use of PyTorch.</p>
<p>I’m all for the framework and the simulator, but it would have been easier if they had just used standard industrial components for the orchestrator (Kubernetes), infrastructure (Terraform) and protocol (Kafka/Nats/etc/etc). Those problems have already been solved. (Correction haha. Once I dug into the getting started guide, it became clear that they are using Kubernetes and Terraform. Great choices! 😂)</p>
<p>The robotics simulator is a collection of <a href="http://mujoco.org/" target="_blank" rel="noopener">MuJoCo</a> simulations. So that is a great addition to the Environment list (despite the licensing terms of MuJoCo).</p>
<p>The RL framework needs a big of coaxing into life. It’s another combination of apt-get’s and conda installs.</p>
<p>Oh wow. I’ve just noticed that they’ve disabled the Github issue tracker. And there’s an explict copyright notice that belongs to each of the authors. OK, so this isn’t even open source.</p>
<p>But the robosuite is MIT licenced?</p>
<p>Very strange. Stopping here due to the lack of issues and werid licensing.</p>
<h2 id="RLgraph"><a href="#RLgraph" class="headerlink" title="RLgraph"></a><a href="https://github.com/rlgraph/rlgraph" target="_blank" rel="noopener">RLgraph</a></h2><p><img src="https://img.shields.io/github/stars/rlgraph/rlgraph.svg?style=flat-square" alt><br><img src="https://img.shields.io/github/issues-closed-raw/rlgraph/rlgraph.svg?style=flat-square" alt><br><img src="https://img.shields.io/github/contributors/rlgraph/rlgraph.svg?style=flat-square" alt><br><img src="https://img.shields.io/github/last-commit/rlgraph/rlgraph.svg?style=flat-square" alt><br><img src="https://img.shields.io/github/commit-activity/y/rlgraph/rlgraph.svg?style=flat-square" alt></p>
<p>So let’s start by saying RLgraph has a whopping number of commits. They’re running at 4000 commits per year. Compare that to OpenAI Gym at just 221. Somebody needs to tell these five people to have a holiday. And it’s only a year old. I can only imagine that it is being used full time.</p>
<p>But anyway. Like other frameworks they are focusing on scalability. But interestingly they are mapping directly to both Tensorflow and Pytorch. They are not using Keras. So that must have been a massive challenge in itself. It looks like they’ve used the <a href="##RLLib\ via\ ray-project">Ray project</a> to distribute work like <a href="http://localhost:1313/a-comparison-of-reinforcement-learning-frameworks/#slm-lab-https-github-com-kengz-slm-lab" target="_blank" rel="noopener">SLM-Lab</a>.</p>
<p>But hurray! They have a pip installer. The configuration of the agents is controlled via JSON. But only the configuration. Not the construction.</p>
<p>I’ve just read that the authors <a href="https://rlgraph.github.io/rlgraph/2019/01/04/introducing-rlgraph.html" target="_blank" rel="noopener">also worked on Tensorforce</a>, which explains some of the de ja voux I have been feeling. And I love that my complaints in Tensorforce, about how the underlying DL framework often leaks into the RL implementation code, have been addressed in RLgraph. I get the feeling that they’ve been listening to me rant to my bored wife.</p>
<blockquote>
<p>Separating spaces of tensors from logical composition enables us to reuse components without ever manually dealing with incompatible shapes again. Note how the above code does not contain any framework-specific notions but only defines an input dataflow from a set of spaces.</p>
</blockquote>
<p>Just want I always wanted. And this is achieved with <a href="https://rlgraph.readthedocs.io/en/latest/spaces.html" target="_blank" rel="noopener">an abstraction of inputs and outputs</a>. Other than that, the API is familiar. An <a href="https://rlgraph.readthedocs.io/en/latest/environments.html" target="_blank" rel="noopener"><code>Environment</code></a> and an <a href="https://github.com/rlgraph/rlgraph/blob/master/rlgraph/agents/agent.py#L42" target="_blank" rel="noopener"><code>Agent</code></a>. There’s a very cool <a href="https://rlgraph.readthedocs.io/en/latest/components.html" target="_blank" rel="noopener"><code>Component</code></a> class that abstracts the DL building blocks.</p>
<p>However, there are abstractions that are missing here. There’s no Policy abstractions. No exploration abstractions. Basically all the nice abstractions from <a href="##coach">Nervana Systems Coach</a>.</p>
<p>But I’m still pretty impressed.</p>
<h3 id="Getting-Started-11"><a href="#Getting-Started-11" class="headerlink" title="Getting Started"></a>Getting Started</h3><p>I altered the cartpole getting started example a little to use the SingleThreadedWorker and enable rendering on the environment to get the video output. Other than that it all looks very familiar.</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> rlgraph.agents <span class="keyword">import</span> DQNAgent</span><br><span class="line"><span class="keyword">from</span> rlgraph.environments <span class="keyword">import</span> OpenAIGymEnv</span><br><span class="line"><span class="keyword">from</span> rlgraph.execution <span class="keyword">import</span> SingleThreadedWorker</span><br><span class="line"></span><br><span class="line">environment = OpenAIGymEnv(<span class="string">'CartPole-v0'</span>, monitor=<span class="string">"."</span>, monitor_video=<span class="number">1</span>, visualize=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Create from .json file or dict, see agent API for all</span></span><br><span class="line"><span class="comment"># possible configuration parameters.</span></span><br><span class="line">agent = DQNAgent.from_file(</span><br><span class="line">  <span class="string">"configs/dqn_cartpole.json"</span>,</span><br><span class="line">  state_space=environment.state_space,</span><br><span class="line">  action_space=environment.action_space</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">episode_returns = []</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">episode_finished_callback</span><span class="params">(episode_return, duration, timesteps, **kwargs)</span>:</span></span><br><span class="line">  episode_returns.append(episode_return)</span><br><span class="line">  <span class="keyword">if</span> len(episode_returns) % <span class="number">10</span> == <span class="number">0</span>:</span><br><span class="line">    print(<span class="string">"Episode &#123;&#125; finished: reward=&#123;:.2f&#125;, average reward=&#123;:.2f&#125;."</span>.format(</span><br><span class="line">      len(episode_returns), episode_return, np.mean(episode_returns[<span class="number">-10</span>:])</span><br><span class="line">    ))</span><br><span class="line"></span><br><span class="line">worker = SingleThreadedWorker(env_spec=<span class="keyword">lambda</span>: environment, agent=agent, render=<span class="literal">True</span>, worker_executes_preprocessing=<span class="literal">False</span>,</span><br><span class="line">                              episode_finish_callback=episode_finished_callback)</span><br><span class="line">print(<span class="string">"Starting workload, this will take some time for the agents to build."</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Use exploration is true for training, false for evaluation.</span></span><br><span class="line">worker.execute_timesteps(<span class="number">1000</span>, use_exploration=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure>
<h2 id="Simple-RL"><a href="#Simple-RL" class="headerlink" title="Simple RL"></a><a href="https://github.com/david-abel/simple_rl" target="_blank" rel="noopener">Simple RL</a></h2><p><img src="https://img.shields.io/github/stars/david-abel/simple_rl.svg?style=flat-square" alt><br><img src="https://img.shields.io/github/issues-closed-raw/david-abel/simple_rl.svg?style=flat-square" alt><br><img src="https://img.shields.io/github/contributors/david-abel/simple_rl.svg?style=flat-square" alt><br><img src="https://img.shields.io/github/last-commit/david-abel/simple_rl.svg?style=flat-square" alt><br><img src="https://img.shields.io/github/commit-activity/y/david-abel/simple_rl.svg?style=flat-square" alt></p>
<p>And finally, simple_rl. All other frameworks state that their goals are performance/scalability or modularity or repeatability. None of them set out to be simple. This is where simple_rl steps in. Built from the ground up to be as simple as possible. It only has two dependencies, <code>numpy</code> and <code>matplotlib</code>. And that’s only if you want to plot the results. Basically it’s just <code>numpy</code>. It has <a href="https://github.com/david-abel/simple_rl#installation" target="_blank" rel="noopener">pip installer</a>. The <a href="https://david-abel.github.io/simple_rl/docs/index.html" target="_blank" rel="noopener">documentation is non-existant</a> but that’s ok, who needs docs? ;-)</p>
<p>It presents a familiar collection of abstractions: an <a href="https://github.com/david-abel/simple_rl/blob/master/simple_rl/agents/AgentClass.py" target="_blank" rel="noopener"><code>agent</code></a>, an <a href="https://github.com/david-abel/simple_rl/blob/master/simple_rl/experiments/ExperimentClass.py" target="_blank" rel="noopener"><code>experiment</code></a>, an environment that is called an <a href="https://github.com/david-abel/simple_rl/blob/master/simple_rl/mdp/MDPClass.py" target="_blank" rel="noopener"><code>mdp</code></a>. The framework also abstracts <a href="https://github.com/david-abel/simple_rl/tree/master/simple_rl/abstraction" target="_blank" rel="noopener">other parts of the model</a> like an action, feature, state. And a planning class that implements the strategies for next actions. It is still very modular, but some of the naming convensions should be changed to match the other frameworks (standardisation).</p>
<p>So it is becoming apparent that “simple” doesn’t necesarily mean easy to understand. Generally, more abstraction makes it harder to understand. Simple in this case is “ease of use”. I think that’s a shame. I really was hoping for simplicity in terms of undertanding. But it looks like it is aiming to compete with some of the more complex frameworks; <a href="https://github.com/david-abel/simple_rl#in-development" target="_blank" rel="noopener">Deep RL support with PyTorch is in development</a>.</p>
<p>There continues to be a gap in the framework market for a very simple, understandable RL framework. And I’m not sure why this framework has so few stars compared to the rest. Presumably because it’s not bootstrapping on the popularity of other DL frameworks, like many of the other frameworks.</p>
<h3 id="Getting-Started-12"><a href="#Getting-Started-12" class="headerlink" title="Getting Started"></a>Getting Started</h3><p>It should have been simple. But deep inside the code there are a few lines that force the <code>Matplotlib</code> to use the <code>TkAgg</code> backend. I tried to get <code>TkAgg</code> working in the Notebook but could not. It is designed for graphical desktop use, so you can imagine that it is not straightforward. I created an <a href="https://github.com/david-abel/simple_rl/issues/40" target="_blank" rel="noopener">issues here</a>. It should be a simple fix.</p>
<p>If/When it works, it should be as simple as something like:</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> simple_rl.agents <span class="keyword">import</span> QLearningAgent, RandomAgent, RMaxAgent</span><br><span class="line"><span class="keyword">from</span> simple_rl.tasks <span class="keyword">import</span> GridWorldMDP</span><br><span class="line"><span class="keyword">from</span> simple_rl.run_experiments <span class="keyword">import</span> run_agents_on_mdp</span><br><span class="line"></span><br><span class="line"><span class="comment"># Setup MDP.</span></span><br><span class="line">mdp = GridWorldMDP(width=<span class="number">4</span>, height=<span class="number">3</span>, init_loc=(<span class="number">1</span>, <span class="number">1</span>), goal_locs=[(<span class="number">4</span>, <span class="number">3</span>)], lava_locs=[(<span class="number">4</span>, <span class="number">2</span>)], gamma=<span class="number">0.95</span>, walls=[(<span class="number">2</span>, <span class="number">2</span>)], slip_prob=<span class="number">0.05</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Setup Agents.</span></span><br><span class="line">ql_agent = QLearningAgent(actions=mdp.get_actions())</span><br><span class="line">rmax_agent = RMaxAgent(actions=mdp.get_actions())</span><br><span class="line">rand_agent = RandomAgent(actions=mdp.get_actions())</span><br><span class="line"></span><br><span class="line"><span class="comment"># Run experiment and make plot.</span></span><br><span class="line">run_agents_on_mdp([ql_agent, rmax_agent, rand_agent], mdp, instances=<span class="number">5</span>, episodes=<span class="number">50</span>, steps=<span class="number">10</span>)</span><br></pre></td></tr></table></figure>
<p>This trains a few different agents and produces a reward plot for each. Nice eh! The only thing I would suggest is that there shouldn’t be any environment implementation in simple_rl. That is out of scope. Leave that to Gym-like projects. For example, <a href="https://github.com/maximecb/gym-minigrid" target="_blank" rel="noopener">gym-minigrid</a> has an awesome Gridworld implementation.</p>
<h1 id="Google-Rankings"><a href="#Google-Rankings" class="headerlink" title="Google Rankings"></a>Google Rankings</h1><p>Google’s Trends search tool allows you to find out what search queries are the most popular. Unfortunately they only provide relative measures and those change depending on what you are querying. Also, common words often get mixed into other queries. For example, searching for “Facebook Horizon” is mixed with a bunch of unrelated queries about “Forza Horizon 4” and “facebook log in”; clearly this inflates this score and cannot be trusted.</p>
<p>I went through all of these frameworks and found that only two frameworks stood out, openai gym and google dopamine. But even for google dopamine, the related queries were google docs/scholar/translate/etc., so I’m not sure if I can trust this either.</p>
<p>One of the things that stood out to me most was the geographical popularity. OpenAI Gym seemed to be the most popular search term, given that it has a high ranking score and all of the related queries are related to RL. But when you look at the how the ranking alters by geography, China is the country with the most searches.</p>
<p>This strikes me as odd, because <a href="https://en.wikipedia.org/wiki/Google_China" target="_blank" rel="noopener">Google is banned in China</a> and so how are they generating these statistics? Are users using VPNs and then searching, and Google is able to recognise that the original traffic is from China?</p>
<h2 id="Don’t-Trust-Google-Trends"><a href="#Don’t-Trust-Google-Trends" class="headerlink" title="Don’t Trust Google Trends"></a>Don’t Trust Google Trends</h2><p>All of this brings me to the conclusion that I can’t trust Google trends at all. OpenAI Gym does seem like the highest ranking RL related framework, which you might expect, but the bulk of that score is coming from China. But Google is blocked in China. Sooo..</p>
<p><a href="https://winderresearch.com/a-comparison-of-reinforcement-learning-frameworks-dopamine-rllib-keras-rl-coach-trfl-tensorforce-coach-and-more/#keras-rl-https-github-com-keras-rl-keras-rl" target="_blank" rel="noopener">原文地址</a></p>

      
    </div>
    
    
    

    

    

    
      <div>
        <ul class="post-copyright">
  <li class="post-copyright-author">
    <strong>本文作者：</strong>
    Glory
  </li>
  <li class="post-copyright-link">
    <strong>本文链接：</strong>
    <a href="http://zanderchang.github.io/2019/12/24/常见强化学习框架比较/" title="常见强化学习框架比较">http://zanderchang.github.io/2019/12/24/常见强化学习框架比较/</a>
  </li>
  <li class="post-copyright-license">
    <strong>版权声明： </strong>
    本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" rel="external nofollow" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明出处！
  </li>
</ul>

      </div>
    

    <footer class="post-footer">
      
        <div class="post-tags">
          
            <a href="/tags/外文翻译/" rel="tag"># 外文翻译</a>
          
            <a href="/tags/强化学习/" rel="tag"># 强化学习</a>
          
        </div>
      

      
      
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/2019/12/11/libfuzzer中的变异策略分析/" rel="next" title="LibFuzzer中的变异策略分析">
                <i class="fa fa-chevron-left"></i> LibFuzzer中的变异策略分析
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
              <a href="/2019/12/29/模糊测试相关资料收集/" rel="prev" title="模糊测试相关资料收集">
                模糊测试相关资料收集 <i class="fa fa-chevron-right"></i>
              </a>
            
          </div>
        </div>
      

      
      
    </footer>
  </div>
  
  
  
  </article>



    <div class="post-spread">
      
    </div>
  </div>


          </div>
          


          

  



        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">
            文章目录
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview-wrap">
            站点概览
          </li>
        </ul>
      

      <section class="site-overview-wrap sidebar-panel">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <p class="site-author-name" itemprop="name">Glory</p>
              <p class="site-description motion-element" itemprop="description"></p>
          </div>

          <nav class="site-state motion-element">

            
              <div class="site-state-item site-state-posts">
              
                <a href="/archives/">
              
                  <span class="site-state-item-count">22</span>
                  <span class="site-state-item-name">日志</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-categories">
                <a href="/categories/index.html">
                  <span class="site-state-item-count">17</span>
                  <span class="site-state-item-name">分类</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-tags">
                <a href="/tags/index.html">
                  <span class="site-state-item-count">23</span>
                  <span class="site-state-item-name">标签</span>
                </a>
              </div>
            

          </nav>

          
            <div class="feed-link motion-element">
              <a href="/atom.xml" rel="alternate">
                <i class="fa fa-rss"></i>
                RSS
              </a>
            </div>
          

          
            <div class="links-of-author motion-element">
                
                  <span class="links-of-author-item">
                    <a href="https://github.com/ZanderChang" target="_blank" title="GitHub">
                      
                        <i class="fa fa-fw fa-github"></i>GitHub</a>
                  </span>
                
                  <span class="links-of-author-item">
                    <a href="mailto:zhangzheng232@hotmail.com" target="_blank" title="E-Mail">
                      
                        <i class="fa fa-fw fa-envelope"></i>E-Mail</a>
                  </span>
                
            </div>
          

          
          

          
          

          

        </div>
      </section>

      
      <!--noindex-->
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
              
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#Original-Purpose-of-this-Work"><span class="nav-number">1.</span> <span class="nav-text">Original Purpose of this Work</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Methodology"><span class="nav-number">2.</span> <span class="nav-text">Methodology</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Accompanying-Notebook"><span class="nav-number">3.</span> <span class="nav-text">Accompanying Notebook</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Reinforcement-Learning-Frameworks"><span class="nav-number">4.</span> <span class="nav-text">Reinforcement Learning Frameworks</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#OpenAI-Gym"><span class="nav-number">4.1.</span> <span class="nav-text">OpenAI Gym</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Getting-Started"><span class="nav-number">4.2.</span> <span class="nav-text">Getting Started</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Google-Dopamine"><span class="nav-number">4.3.</span> <span class="nav-text">Google Dopamine</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Getting-Started-1"><span class="nav-number">4.3.1.</span> <span class="nav-text">Getting Started</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#RLLib-via-ray-project"><span class="nav-number">4.4.</span> <span class="nav-text">RLLib via ray-project</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Getting-Started-2"><span class="nav-number">4.4.1.</span> <span class="nav-text">Getting Started</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Keras-RL"><span class="nav-number">4.5.</span> <span class="nav-text">Keras-RL</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Getting-Started-3"><span class="nav-number">4.5.1.</span> <span class="nav-text">Getting Started</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#TRFL"><span class="nav-number">4.6.</span> <span class="nav-text">TRFL</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Getting-Started-4"><span class="nav-number">4.6.1.</span> <span class="nav-text">Getting Started</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Tensorforce"><span class="nav-number">4.7.</span> <span class="nav-text">Tensorforce</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Getting-Started-5"><span class="nav-number">4.7.1.</span> <span class="nav-text">Getting Started</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Facebook-Horizon"><span class="nav-number">4.8.</span> <span class="nav-text">Facebook Horizon</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Getting-Started-6"><span class="nav-number">4.8.1.</span> <span class="nav-text">Getting Started</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Nervana-Systems-Coach"><span class="nav-number">4.9.</span> <span class="nav-text">Nervana Systems Coach</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Getting-Started-7"><span class="nav-number">4.10.</span> <span class="nav-text">Getting Started</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#MAgent"><span class="nav-number">4.11.</span> <span class="nav-text">MAgent</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Getting-Started-8"><span class="nav-number">4.11.1.</span> <span class="nav-text">Getting Started</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#TF-Agents"><span class="nav-number">4.12.</span> <span class="nav-text">TF-Agents</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Getting-Started-9"><span class="nav-number">4.12.1.</span> <span class="nav-text">Getting Started</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#SLM-Lab"><span class="nav-number">4.13.</span> <span class="nav-text">SLM-Lab</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#DeeR"><span class="nav-number">4.14.</span> <span class="nav-text">DeeR</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Getting-Started-10"><span class="nav-number">4.14.1.</span> <span class="nav-text">Getting Started</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Garage"><span class="nav-number">4.15.</span> <span class="nav-text">Garage</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Surreal"><span class="nav-number">4.16.</span> <span class="nav-text">Surreal</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#RLgraph"><span class="nav-number">4.17.</span> <span class="nav-text">RLgraph</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Getting-Started-11"><span class="nav-number">4.17.1.</span> <span class="nav-text">Getting Started</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Simple-RL"><span class="nav-number">4.18.</span> <span class="nav-text">Simple RL</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Getting-Started-12"><span class="nav-number">4.18.1.</span> <span class="nav-text">Getting Started</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Google-Rankings"><span class="nav-number">5.</span> <span class="nav-text">Google Rankings</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Don’t-Trust-Google-Trends"><span class="nav-number">5.1.</span> <span class="nav-text">Don’t Trust Google Trends</span></a></li></ol></li></ol></div>
            

          </div>
        </section>
      <!--/noindex-->
      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; 2019 &mdash; <span itemprop="copyrightYear">2020</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Glory</span>

  
</div>


  <div class="powered-by">由 <a class="theme-link" target="_blank" href="https://hexo.io">Hexo</a> 强力驱动</div>



  <span class="post-meta-divider">|</span>



  <div class="theme-info">主题 &mdash; <a class="theme-link" target="_blank" href="https://github.com/iissnan/hexo-theme-next">NexT.Mist</a> v5.1.4</div>





    <script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>

    <span id="busuanzi_container_site_pv">总访问量<span id="busuanzi_value_site_pv"></span>次</span>
    <span class="post-meta-divider">|</span>
    <span id="busuanzi_container_site_uv">总访客<span id="busuanzi_value_site_uv"></span>人</span>
    <span class="post-meta-divider">|</span>


        







        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  












  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>
  

  
  
    <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.4"></script>



  
  

  
  <script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.4"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.1.4"></script>



  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.4"></script>



  


  




	





  





  












  

  <script type="text/javascript">
    // Popup Window;
    var isfetched = false;
    var isXml = true;
    // Search DB path;
    var search_path = "search.xml";
    if (search_path.length === 0) {
      search_path = "search.xml";
    } else if (/json$/i.test(search_path)) {
      isXml = false;
    }
    var path = "/" + search_path;
    // monitor main search box;

    var onPopupClose = function (e) {
      $('.popup').hide();
      $('#local-search-input').val('');
      $('.search-result-list').remove();
      $('#no-result').remove();
      $(".local-search-pop-overlay").remove();
      $('body').css('overflow', '');
    }

    function proceedsearch() {
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay"></div>')
        .css('overflow', 'hidden');
      $('.search-popup-overlay').click(onPopupClose);
      $('.popup').toggle();
      var $localSearchInput = $('#local-search-input');
      $localSearchInput.attr("autocapitalize", "none");
      $localSearchInput.attr("autocorrect", "off");
      $localSearchInput.focus();
    }

    // search function;
    var searchFunc = function(path, search_id, content_id) {
      'use strict';

      // start loading animation
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay">' +
          '<div id="search-loading-icon">' +
          '<i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>' +
          '</div>' +
          '</div>')
        .css('overflow', 'hidden');
      $("#search-loading-icon").css('margin', '20% auto 0 auto').css('text-align', 'center');

      $.ajax({
        url: path,
        dataType: isXml ? "xml" : "json",
        async: true,
        success: function(res) {
          // get the contents from search data
          isfetched = true;
          $('.popup').detach().appendTo('.header-inner');
          var datas = isXml ? $("entry", res).map(function() {
            return {
              title: $("title", this).text(),
              content: $("content",this).text(),
              url: $("url" , this).text()
            };
          }).get() : res;
          var input = document.getElementById(search_id);
          var resultContent = document.getElementById(content_id);
          var inputEventFunction = function() {
            var searchText = input.value.trim().toLowerCase();
            var keywords = searchText.split(/[\s\-]+/);
            if (keywords.length > 1) {
              keywords.push(searchText);
            }
            var resultItems = [];
            if (searchText.length > 0) {
              // perform local searching
              datas.forEach(function(data) {
                var isMatch = false;
                var hitCount = 0;
                var searchTextCount = 0;
                var title = data.title.trim();
                var titleInLowerCase = title.toLowerCase();
                var content = data.content.trim().replace(/<[^>]+>/g,"");
                var contentInLowerCase = content.toLowerCase();
                var articleUrl = decodeURIComponent(data.url);
                var indexOfTitle = [];
                var indexOfContent = [];
                // only match articles with not empty titles
                if(title != '') {
                  keywords.forEach(function(keyword) {
                    function getIndexByWord(word, text, caseSensitive) {
                      var wordLen = word.length;
                      if (wordLen === 0) {
                        return [];
                      }
                      var startPosition = 0, position = [], index = [];
                      if (!caseSensitive) {
                        text = text.toLowerCase();
                        word = word.toLowerCase();
                      }
                      while ((position = text.indexOf(word, startPosition)) > -1) {
                        index.push({position: position, word: word});
                        startPosition = position + wordLen;
                      }
                      return index;
                    }

                    indexOfTitle = indexOfTitle.concat(getIndexByWord(keyword, titleInLowerCase, false));
                    indexOfContent = indexOfContent.concat(getIndexByWord(keyword, contentInLowerCase, false));
                  });
                  if (indexOfTitle.length > 0 || indexOfContent.length > 0) {
                    isMatch = true;
                    hitCount = indexOfTitle.length + indexOfContent.length;
                  }
                }

                // show search results

                if (isMatch) {
                  // sort index by position of keyword

                  [indexOfTitle, indexOfContent].forEach(function (index) {
                    index.sort(function (itemLeft, itemRight) {
                      if (itemRight.position !== itemLeft.position) {
                        return itemRight.position - itemLeft.position;
                      } else {
                        return itemLeft.word.length - itemRight.word.length;
                      }
                    });
                  });

                  // merge hits into slices

                  function mergeIntoSlice(text, start, end, index) {
                    var item = index[index.length - 1];
                    var position = item.position;
                    var word = item.word;
                    var hits = [];
                    var searchTextCountInSlice = 0;
                    while (position + word.length <= end && index.length != 0) {
                      if (word === searchText) {
                        searchTextCountInSlice++;
                      }
                      hits.push({position: position, length: word.length});
                      var wordEnd = position + word.length;

                      // move to next position of hit

                      index.pop();
                      while (index.length != 0) {
                        item = index[index.length - 1];
                        position = item.position;
                        word = item.word;
                        if (wordEnd > position) {
                          index.pop();
                        } else {
                          break;
                        }
                      }
                    }
                    searchTextCount += searchTextCountInSlice;
                    return {
                      hits: hits,
                      start: start,
                      end: end,
                      searchTextCount: searchTextCountInSlice
                    };
                  }

                  var slicesOfTitle = [];
                  if (indexOfTitle.length != 0) {
                    slicesOfTitle.push(mergeIntoSlice(title, 0, title.length, indexOfTitle));
                  }

                  var slicesOfContent = [];
                  while (indexOfContent.length != 0) {
                    var item = indexOfContent[indexOfContent.length - 1];
                    var position = item.position;
                    var word = item.word;
                    // cut out 100 characters
                    var start = position - 20;
                    var end = position + 80;
                    if(start < 0){
                      start = 0;
                    }
                    if (end < position + word.length) {
                      end = position + word.length;
                    }
                    if(end > content.length){
                      end = content.length;
                    }
                    slicesOfContent.push(mergeIntoSlice(content, start, end, indexOfContent));
                  }

                  // sort slices in content by search text's count and hits' count

                  slicesOfContent.sort(function (sliceLeft, sliceRight) {
                    if (sliceLeft.searchTextCount !== sliceRight.searchTextCount) {
                      return sliceRight.searchTextCount - sliceLeft.searchTextCount;
                    } else if (sliceLeft.hits.length !== sliceRight.hits.length) {
                      return sliceRight.hits.length - sliceLeft.hits.length;
                    } else {
                      return sliceLeft.start - sliceRight.start;
                    }
                  });

                  // select top N slices in content

                  var upperBound = parseInt('1');
                  if (upperBound >= 0) {
                    slicesOfContent = slicesOfContent.slice(0, upperBound);
                  }

                  // highlight title and content

                  function highlightKeyword(text, slice) {
                    var result = '';
                    var prevEnd = slice.start;
                    slice.hits.forEach(function (hit) {
                      result += text.substring(prevEnd, hit.position);
                      var end = hit.position + hit.length;
                      result += '<b class="search-keyword">' + text.substring(hit.position, end) + '</b>';
                      prevEnd = end;
                    });
                    result += text.substring(prevEnd, slice.end);
                    return result;
                  }

                  var resultItem = '';

                  if (slicesOfTitle.length != 0) {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + highlightKeyword(title, slicesOfTitle[0]) + "</a>";
                  } else {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + title + "</a>";
                  }

                  slicesOfContent.forEach(function (slice) {
                    resultItem += "<a href='" + articleUrl + "'>" +
                      "<p class=\"search-result\">" + highlightKeyword(content, slice) +
                      "...</p>" + "</a>";
                  });

                  resultItem += "</li>";
                  resultItems.push({
                    item: resultItem,
                    searchTextCount: searchTextCount,
                    hitCount: hitCount,
                    id: resultItems.length
                  });
                }
              })
            };
            if (keywords.length === 1 && keywords[0] === "") {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-search fa-5x" /></div>'
            } else if (resultItems.length === 0) {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-frown-o fa-5x" /></div>'
            } else {
              resultItems.sort(function (resultLeft, resultRight) {
                if (resultLeft.searchTextCount !== resultRight.searchTextCount) {
                  return resultRight.searchTextCount - resultLeft.searchTextCount;
                } else if (resultLeft.hitCount !== resultRight.hitCount) {
                  return resultRight.hitCount - resultLeft.hitCount;
                } else {
                  return resultRight.id - resultLeft.id;
                }
              });
              var searchResultList = '<ul class=\"search-result-list\">';
              resultItems.forEach(function (result) {
                searchResultList += result.item;
              })
              searchResultList += "</ul>";
              resultContent.innerHTML = searchResultList;
            }
          }

          if ('auto' === 'auto') {
            input.addEventListener('input', inputEventFunction);
          } else {
            $('.search-icon').click(inputEventFunction);
            input.addEventListener('keypress', function (event) {
              if (event.keyCode === 13) {
                inputEventFunction();
              }
            });
          }

          // remove loading animation
          $(".local-search-pop-overlay").remove();
          $('body').css('overflow', '');

          proceedsearch();
        }
      });
    }

    // handle and trigger popup window;
    $('.popup-trigger').click(function(e) {
      e.stopPropagation();
      if (isfetched === false) {
        searchFunc(path, 'local-search-input', 'local-search-result');
      } else {
        proceedsearch();
      };
    });

    $('.popup-btn-close').click(onPopupClose);
    $('.popup').click(function(e){
      e.stopPropagation();
    });
    $(document).on('keyup', function (event) {
      var shouldDismissSearchPopup = event.which === 27 &&
        $('.search-popup').is(':visible');
      if (shouldDismissSearchPopup) {
        onPopupClose();
      }
    });
  </script><!-- hexo-inject:begin --><!-- hexo-inject:end -->





  

  

  

  
  

  
  


  

  

</body>
</html>
