---
title: 强化学习纲要笔记
mathjax: true
date: 2020-05-12 09:35:49
categories:
- 强化学习
tags:
- 强化学习
---

周博磊老师的强化学习纲要课程 10课时

[课程地址](https://space.bilibili.com/511221970/)

[课程资料](https://github.com/zhoubolei/introRL)

May ReinForce Be With You !

<!-- more -->

# 概括与RL基础

- RL特点
  - 试错探索
  - 输入数据有时间关联（非i.i.d）
  - 奖励存在延迟
  - agent行为影响后续数据
- POMDP
- Cross Entropy method (CEM)

# 马尔可夫决策过程

## MP/MRP（小船随波逐流）
- 计算$V(s)$
  - 小规模
    - 矩阵求逆
  - 大规模
    - DP Bootstrap自举迭代Bellman
    - MC 采样取平均
    - TD = DP + MC
## MDP（小船上有船夫）
- Policy Evaluation/(Value) **Prediction**: 计算$v^{\pi}(s)$
  - BEE
- **Control**: 计算$v^*(s)$和$\pi^*$
  - 策略迭代
    - Policy evaluation: BEE
    - Policy improvement: greedy
  - 值迭代
    - BOE

# 无模型的价值函数估计和控制

- MDP未知（R和P未知）
- 交互

## Model-free prediction

- MC
  - empirical mean return
  - 完整episode
  - 增量更新
- TD
  - online 每步更新
  - 不完整episode
  - TD target
  - TD error
  - $\infty$-step TD = MC

||DP|MC|TD|
|-|-|-|-|
|Bootstrap|$\checkmark$||$\checkmark$|
|Sample||$\checkmark$|$\checkmark$|

## Model-free control

- Generalized Policy Iteration (GPI)
  - $Q=q_\pi$
  - $\pi=greedy(Q)$
  - MC/TD
  - $\epsilon$-greedy
- Sarsa
  - Q(S,A)
  - On-Policy TD Control
    - 同一个Policy进行采集和优化
- Q-learning
  - Off-Policy TD Control
  - target $\pi$
    - greedy
  - behaviour $\mu$
    - $\epsilon$-greedy

# 价值函数的近似 VFA

- 函数近似（大规模问题）
  - 线性叠加**特征**
    - 梯度下降
  - 非线性
    - DNN
- prediction
  - Oracle -> $G_t$ / $TD_{target}$
  - MC
    - unbiased but noisy
  - TD
    - biased
    - semi-gradient
- control
  - semi-gradient Sarsa for VFA Control
- RL死亡三角 FA + bootstrap + off-policy
- Batch

- DQN
  - Experience Replay
  - Fixed Target $w^-$
    - 增加稳定性
    - target延时更新
    - 猫（estimate）抓老鼠（target）

[Agent57](https://deepmind.com/blog/article/Agent57-Outperforming-the-human-Atari-benchmark)

![DQN类方法的演进](https://kstatic.googleusercontent.com/files/f6b5f285173d4449285a8e812b8385f45c03f7104e1c41370a73e0c8558ff82d6a69e60962dd91c4972c444fd73bc4f98a06b5487eff5a037a37bc42f97cef3b)

# 策略优化基础（难）

- Policy-based RL
  - $\tau$为采样
  - $\pi_\theta(s,a)$
  - no value function
  - 优势
    - 收敛性更好
    - 高维动作空间上更有效
    - 可以学习随机策略（输出为概率）
      - Rock-Paper-Scissors
      - Aliased Gridworld
  - 劣势
    - 局部最优解
    - 高方差、测试结果不稳定
    - sample效率低（on-policy）
  - 极大化$J(\theta)$
    - 可微分
      - 梯度上升
      - 共轭梯度
      - quasi-newton
    - 不可微分 black-box Derivative-free
      - CEM
      - Hill climbing
      - Evolution alg
  - Policy Example
    - Softmax
    - Gaussian
- MC Policy Gradient
  - Score Function
  - 公式推导

- 减小PG方差
  - Use temporal causality 时序因果关系
    - REINFORCE (1992)
  - Use a baseline $G_t-b_w(s_t)$
    - Vanilla PG (1999)
  - Use a Critic $G_t\rightarrow Q_w(s,a)$
    - AC PG
      - Advantage function (baseline $V$)
        - $A^\pi(s,a) = Q^\pi(s,a) - V^\pi(s)$
      - MC采样解决不可微分的问题

- SOTA RL
  - PG -> TRPO(2015) -> ACKTR(2017) -> PPO(2017)
  - Q-learning -> DDPG(2014) -> TD3(2018) -> SAC(2018)

# 策略优化进阶

- PG
  - 改进思路
    - 训练更稳定
      - Trust Region
        - KL限制$\pi_\theta$和$\pi_{\theta_{old}}$差异
        - 限定区域（球体）并逐渐缩小
      - Natural PG
        - 参数空间$\rightarrow$分布空间（policy输出）
        - KL散度（策略更新前后差异较小）
        - Fisher information matrix (FIM)
        - 二阶优化（比SGD更准确）
        - 策略优化和策略函数的参数化形式独立
    - 提高sample效率 on-policy$\rightarrow$off-policy 
      - TRPO中的重要性采样IS
        - $\pi_\theta/\pi_{\theta_{old}}$
  - TRPO (Trust Region Policy Optimization)
    - MM alg (EM)
    - 存在问题
      - 计算量大
      - 需要样本多
      - Conjugate Gradient (CG)本身较为复杂
      - 部分表现差于DQN
  - ACKTR
    - 提高TRPO的计算效率
      - K-FAC加速FIM求逆
  - PPO
    - TRPO的简化（将约束作为惩罚）应用广泛
    - 一阶优化（SGD）
    - with clipping（简单易实现）
- Q-learning
  - DDPG
    - 将DQN扩展到连续动作空间
  - Twin Delayed DDPG (TD3)
    - DDPG有时会过大估计Q值
    - 改进
      - Clipped Double-Q Learning
        - 2个Q函数取较小值
      - Delayed Policy Update
      - Target Policy Smoothing
        - noise + clip
    - 官方代码非常值得学习
  - Soft Actor-Critic (SAC)
    - **Entropy**-regularized RL
    - Reparameterization Trick

# 基于环境模型的RL方法

- 简介
  - 学习环境模型
  - Plan
  - sample效率高（现实应用中非常重要）
  - 难以收敛、2个误差
- Model-based value optimization
  - model -> simulated trajectoried -> values -> policy
  - Model
    - Table Lookup 计数
  - Dyna(1991)
    - 用少量真实轨迹估计模型
- Model-based policy optimization
  - model -> policy
  - Optimal Control
    - LQR/iLQR
  - MPC
- 案例 Robotic Object Manipulation
  - PILCO(2011)
  - (2015)

# 模仿学习IL

- 简介
  - policy network的监督学习
- Behavioral cloning (BC)
  - off-course situation中表现差
- DAgger: Dataset Aggregation
  - 使数据分布尽可能一致
  - 人工标记 -> 其它算法来标记
- Inverse RL (IRL)
  - $R_\theta(s,a)$
  - Guided Cost Learning (2016)
- GAIL: Generative Adversarial IL (2016)
  - 类比GAN的思想生成轨迹
- 进一步改进
  - Multimodal/Non-Markovian behavior
  - 多峰高斯输出
- 结合IL和RL
  - Pretrain & Finetune
  - Off-policy
  - IL as an auxiliary loss function
- 案例
  - BC 自动驾驶、无人机
  - IL LSTM (2018)
  - Motion Imitation (2018)
    - PPO
    - 去掉MoCap数据

# RL分布式系统

- 分布式ML
  - 分布式操作系统 MIT EECS 6.824
  - Consistency, Fault tolerance, Communication
  - Model/Data parallelism
  - Sync/Async Update
  - Hogwild(2011)
    - Lock-free async SGD
  - Jeff Dean
    - MapReduce(2004)
    - DisBelief(2012)
  - AlexNet(2012)
- 分布式RL
  - DQN(2013)
  - GORILA(2015)
  - A3C(2016)
    - async
    - CPU多线程actor
  - A2C(2017)
    - sync
    - GPU
  - Ape-X(2018)
    - Distributed DQN/DDPG
  - IMPALA(2018)
    - actor只产生experience而不是gradient
    - IS
  - RLLib(2018)
    - 将不同算法模块化 reuse
  - Evolution Strategies(2017)
- 案例
  - AlphaGo
  - OpenAI Five
  - AlphaStar
    - PPO
    - Rapid训练框架

# 完结篇

RL in a nutshell

- Basics of RL
- MDP and tabular solution methods
- 价值函数近似
- 策略优化
  - Log Derivative Trick vs. Reparameterization Trick
  - [Spinning-Up](https://spinningup.openai.com) 各种RL算法的原理和实现
- 其它主题
  - Model-based RL 效率高、存在2处近似误差
  - 模仿学习 policy的监督学习
  - 分布式系统
- Open problems
  - Sample efficiency
  - Generalist RL rather than specialist RL
  - New env and agent designs
    - [ml-agents](https://github.com/Unity-Technologies/ml-agents)
  - Bridge RL with other ML topics
    - Yann LeCun's cake

# 番外 剖析星际争霸AI AlphaStar

Nature

- 简介
- 环境设计
  - 状态 4
    - 实体信息（链表向量）（不定长）
    - 地图信息（图像）
    - 玩家数据和游戏设计（标量）
  - 动作 6 Autoregressive
    - 层级解耦
- 网络结构
  - MLP Transformer ResNet（处理输入）
  - Deep LSTM（core）
  - 指针网络
  - 串行输出
- 监督学习
  - 解决网络初始化问题
- 强化学习
  - off-policy
  - AC结构，基于IMPALA
  - 重要性采样
- RL - V-trace
  - 限制重要性采样系数，解决Off-policy
  - 来自IMPALA
- RL - UPGO Upgoing Policy Update
  - 解决优势估算的问题 $G_t^U$
  - GAE来自PPO
- RL - TD($\lambda$)
  - 同时输入对手数据
- 模仿学习
  - 注入人类信息，助力RL
  - 人类统计量$Z$来计算伪奖励函数（胜负奖励之外）75%概率置0
    - 编辑距离 汉明距离（01不同）
- 自学习 Self-play
  - 策略循环问题
    - 虚拟自学习 Fictitious Self-Play FSP 存档得到种群，从中**均匀随机**选择
      - 对手太菜时浪费时间
    - Prioritized FSP
- 联盟训练 League Training（最大创新）探索policy空间
  - 对手池 = 联盟
    - 主智能体 3 50% 35% 15% 永不重置 主要对象
    - 联盟利用者 6 发现全局弱点
    - 主利用者 3 发现主智能体弱点
- 实验结果
  - 消融实验 Ablation Study 比较各部分的重要性
    - 人类数据极其重要
- 总结（成功经验）
  - 高度复杂的神经网络融合了列表、图像、标量信息等输⼊
  - 通过Autoregressive的网络设计解耦了结构化的动作空间
  - 模仿学习和监督学习的成功运用（统计量量Z等）
  - 复杂的强化学习算法
  - 复杂的联盟训练策略
  - 大量计算资源（40d+）